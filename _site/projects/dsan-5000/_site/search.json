[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning is a type of machine learning that analyzes data for patterns and relationships without having any data points’ labels. To put it simply, unsupervised learning “blindly” tries to make assumptions about data points’ associations with one another by assessing their similarities when their dimensionality is boiled down. This is different from supervised learning, which already knows the labels of data points, but learns to build a model to predict this labeled variable by assessing its relevance to other features. For this unsupervised learning section, I will first perform dimensionality reduction, which allows for preliminary visualization of the underlying relationships between data points. Next, I will perform three different methods of unsupervised clustering: K-means, Density-Based Spatial Clustering Applications with Noise (DBSCAN), and Hierarchical clustering.\n\n# Load dataset\nimport pandas as pd\ndf = pd.read_csv('../../data/processed-data/MLdataset.csv')"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca",
    "href": "technical-details/unsupervised-learning/main.html#pca",
    "title": "Unsupervised Learning",
    "section": "PCA",
    "text": "PCA\nPrinciple Component Analysis (PCA) is a common data preprocessing step which reduces the number of dimensions in large datasets by transforming seemingly correlated variables into smaller sets of variables (principle components). The result is a smaller set of dimensions that can retain most of the original information. PCA is important for computing efficiency and impactful for identifying trends, patterns or outliers in high-dimensional data. See IBM’s definition for further information.\nIn the code below, I will: 1. Prepare the data for dimensionality reduction by handling categorical variables, fill missing values with the mean value, and scale the data using Sci-kit learn’s StandardScaler (this is a python package, and its tool automatically applies z-score standardization to the data, which reduces all data points to a more standard and comparable metric) 2. Write functions to: - create a plot that visualizes the amount of variance explained with each incremental additional included component (variable) - calculate the optimal number of clusters using the silhouette score on the PCA transformed data - create a scatter plot that visualizes how similar data points are to each other that are in the same cluster and how distinguished the different clusters are\n\nimport pandas as pd             \nimport numpy as np                \nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.impute import SimpleImputer  \nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score  \nimport seaborn as sns        \n\n\ndef prepare_data(df):\n    # One-hot encode the categorical variables (turns them into numerical 0 or 1)\n    df_encoded = pd.get_dummies(df, columns=['Subregion', 'prostitution_policy'])\n    \n    # Drop non-numeric columns\n    df_encoded = df_encoded.drop(['Country'], axis=1)\n    \n    # Handle missing values\n    df_encoded = df_encoded.fillna(df_encoded.mean())\n    \n    # Scale\n    scaler = StandardScaler()\n    X = scaler.fit_transform(df_encoded)\n    \n    return X, df_encoded.columns\n\n# analyses the optimal number of PCA components (the number of features to include)\ndef analyze_pca(X):\n    pca = PCA()\n    pca.fit(X)\n    \n    # cumulative variance ratio\n    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n    \n    # Plot explained variance\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(cumulative_variance_ratio) + 1), \n             cumulative_variance_ratio, 'bo-')\n    plt.axhline(y=0.8, color='r', linestyle='--', label='80% explained variance')\n    plt.axhline(y=0.95, color='g', linestyle='--', label='95% explained variance')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance Ratio')\n    plt.title('PCA Explained Variance')\n    plt.legend()\n    plt.show()\n    \n    # Two different thresholds are used : .80 vs 0.95\n    # Though an 80% explained variance is quite low, this is complex data and I wanted to be able to compare to \n    # understand how much explained variance is lost with fewer components included\n    n_components_80 = np.argmax(cumulative_variance_ratio &gt;= 0.8) + 1 \n    n_components_95 = np.argmax(cumulative_variance_ratio &gt;= 0.95) + 1\n    \n    print(f\"Number of components needed for 80% variance: {n_components_80}\")\n    print(f\"Number of components needed for 95% variance: {n_components_95}\")\n    \n    return n_components_80, n_components_95\n\n# function to identify the optimal number of clusters to visualize using the elbow method and silhouette score \n\ndef find_optimal_clusters(X_pca, min_clusters=2, max_clusters=10):\n    silhouette_scores = []\n    cluster_range = range(min_clusters, max_clusters + 1)\n    \n    for k in cluster_range:\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        labels = kmeans.fit_predict(X_pca)\n        score = silhouette_score(X_pca, labels)\n        silhouette_scores.append(score)\n        print(f\"Number of clusters: {k}, Silhouette score: {score:.4f}\")\n    \n    # Find the number of clusters with the max silhouette score\n    optimal_k = cluster_range[np.argmax(silhouette_scores)]\n    print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n    \n    # silhouette scores\n    plt.figure(figsize=(10, 6))\n    plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='--')\n    plt.title('Silhouette Score for Different Numbers of Clusters')\n    plt.xlabel('Number of Clusters')\n    plt.ylabel('Silhouette Score')\n    plt.show()\n    \n    return optimal_k\n    \n\n\n\n# function to run both of the above and output results\ndef pca_analysis(df):\n    # output annotations\n    print(\"1. Preparing and standardizing data...\")\n    X_scaled, feature_names = prepare_data(df)  # Unpack both returned values\n    \n    print(\"\\n2. Analyzing optimal number of components...\")\n    # analyze number of components needed\n    pca_full = PCA()\n    pca_full.fit(X_scaled)\n    cumulative_variance_ratio = np.cumsum(pca_full.explained_variance_ratio_)\n    \n    # Plot explaine variance\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(cumulative_variance_ratio) + 1), \n             cumulative_variance_ratio, 'bo-')\n    plt.axhline(y=0.8, color='r', linestyle='--', label='80% explained variance')\n    plt.axhline(y=0.95, color='g', linestyle='--', label='95% explained variance')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance Ratio')\n    plt.title('PCA Explained Variance')\n    plt.legend()\n    plt.show()\n    \n    print(\"\\n3. Performing PCA transformation for visualization...\")\n    # 2D for visualization\n    pca_2d = PCA(n_components=2)\n    X_pca = pca_2d.fit_transform(X_scaled)\n    print(f\"Explained variance ratios for first two components: {pca_2d.explained_variance_ratio_}\")\n    \n    # First plot: Basic PCA scatter plot without clustering\n    plt.figure(figsize=(10, 8))\n    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.title('PCA: First Two Principal Components')\n    plt.grid(True)\n    plt.show()\n    \n    print(\"\\n4. Finding optimal number of clusters...\")\n    opt_k = find_optimal_clusters(X_pca)\n    print(f\"Optimal number of clusters: {opt_k}\")\n    \n    print(\"\\n5. Performing K-means clustering...\")\n    kmeans = KMeans(n_clusters=opt_k, random_state=6547)\n    cluster_labels = kmeans.fit_predict(X_pca)\n    \n    # Second plot: PCA with cluster assignments\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                         c=cluster_labels, cmap='viridis', \n                         alpha=0.7)\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.title(f'PCA with {opt_k} Clusters')\n    plt.colorbar(scatter)\n    plt.grid(True)\n    plt.show()\n    \n    results = {\n        'X_scaled': X_scaled,\n        'X_pca': X_pca,\n        'pca': pca_2d,\n        'cluster_labels': cluster_labels,\n        'optimal_k': opt_k,\n        'explained_variance_ratio': pca_2d.explained_variance_ratio_\n    }\n    \n    return results\n\npca_results = pca_analysis(df)\n\n1. Preparing and standardizing data...\n\n2. Analyzing optimal number of components...\n\n\n\n\n\n\n\n\n\n\n3. Performing PCA transformation for visualization...\nExplained variance ratios for first two components: [0.16484017 0.11272867]\n\n\n\n\n\n\n\n\n\n\n4. Finding optimal number of clusters...\nNumber of clusters: 2, Silhouette score: 0.3721\nNumber of clusters: 3, Silhouette score: 0.4892\nNumber of clusters: 4, Silhouette score: 0.4150\nNumber of clusters: 5, Silhouette score: 0.4281\nNumber of clusters: 6, Silhouette score: 0.4086\nNumber of clusters: 7, Silhouette score: 0.3730\nNumber of clusters: 8, Silhouette score: 0.3758\nNumber of clusters: 9, Silhouette score: 0.4114\nNumber of clusters: 10, Silhouette score: 0.3952\n\nOptimal number of clusters based on silhouette score: 3\n\n\n\n\n\n\n\n\n\nOptimal number of clusters: 3\n\n5. Performing K-means clustering...\n\n\n\n\n\n\n\n\n\nThe explained variance plot indicates that when reduced to 13 principle components, 80% of the variance in the data can be explained, and 95% of the variance in the data can be explained when approximately 22 principle components are included. This suggests that the data is highly complex and there are not clear, discernible relationships between data points– the variance cannot be explained with only a few highly representative points."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne",
    "href": "technical-details/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nT-Distributed Stochastic Neighbors, or t-SNE, is another essential unsupervised dimensionality reduction technique. It differs from PCA in that it is non-linear– while PCA depends on linear structure to minimize variances and preserve larger, global pariwise distances, t-SNE focuses more on using other techniques to preserve smaller, more local pairwise distances. t-SNE allows for the visualization of high-dimensional data by converting the relationships between two data points into joint probabilities and minimizing the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data– essentially, it tries to compare data points as if they are distributions. The underlying methods are quite complex, but Datacamp and Scikit-learn.org’s explanations are very helpful for understanding the tool.\nIn the code below, I will: - transform the data to t-SNE - visualize the t-SNE plots with varying degrees of perplexity (perplexity is a hyperparameter, or key influencing characteristic of t-SNE, which is commonly defined as quantifying the the balance between preserving the global and the local structure of the data. Therefore, by varying the perplexity, the plots should show different physical clustering characteristics). - calculate the optimal number of clusters using the same silhouette score-based function from PCA - plot the t-SNE shaded with the optimal number of clusters identified\n\nfrom sklearn.manifold import TSNE\n\ndef tsne_analysis(df):\n    # add annotations to accompany the outputs: \n    print(\"1. Preparing and standardizing data...\")\n    X_scaled, feature_names = prepare_data(df)\n    \n    print(\"\\n2. Testing different perplexity values...\")\n    # test different perplexity values\n    perplexities = [5, 25, 40]\n    tsne_results = {}\n    \n    fig, axes = plt.subplots(1, len(perplexities), figsize=(20, 6))\n    \n    for idx, perp in enumerate(perplexities):\n        print(f\"Computing t-SNE with perplexity={perp}...\")\n        tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n        tsne_result = tsne.fit_transform(X_scaled)\n        \n        # Store \n        tsne_results[f'perplexity_{perp}'] = tsne_result\n        \n        # Plot\n        axes[idx].scatter(tsne_result[:, 0], tsne_result[:, 1])\n        axes[idx].set_title(f't-SNE (perplexity={perp})')\n        axes[idx].set_xlabel('t-SNE 1')\n        axes[idx].set_ylabel('t-SNE 2')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n3. Using perplexity=25 for further analysis...\")\n    # chose 25 because it's in the middle\n    X_tsne = tsne_results['perplexity_25']\n    \n    print(\"\\n4. Finding optimal number of clusters...\")\n    opt_k = find_optimal_clusters(X_tsne)\n    print(f\"Optimal number of clusters: {opt_k}\")\n    \n    print(\"\\n5. Performing K-means clustering...\")\n    kmeans = KMeans(n_clusters=opt_k, random_state=6547)\n    cluster_labels = kmeans.fit_predict(X_tsne)\n    \n    # Plot t-SNE with cluster assignments\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n                         c=cluster_labels, cmap='viridis', \n                         alpha=0.7)\n    plt.xlabel('t-SNE 1')\n    plt.ylabel('t-SNE 2')\n    plt.title(f't-SNE with {opt_k} Clusters')\n    plt.colorbar(scatter)\n    plt.grid(True)\n    plt.show()\n    \n    results = {\n        'X_scaled': X_scaled,\n        'tsne_results': tsne_results,\n        'X_tsne': X_tsne,\n        'cluster_labels': cluster_labels,\n        'optimal_k': opt_k\n    }\n    \n    return results\n\n# Usage:\ntsne_results = tsne_analysis(df)\n\n1. Preparing and standardizing data...\n\n2. Testing different perplexity values...\nComputing t-SNE with perplexity=5...\nComputing t-SNE with perplexity=25...\nComputing t-SNE with perplexity=40...\n\n\n\n\n\n\n\n\n\n\n3. Using perplexity=25 for further analysis...\n\n4. Finding optimal number of clusters...\nNumber of clusters: 2, Silhouette score: 0.3900\nNumber of clusters: 3, Silhouette score: 0.3288\nNumber of clusters: 4, Silhouette score: 0.3529\nNumber of clusters: 5, Silhouette score: 0.3727\nNumber of clusters: 6, Silhouette score: 0.3548\nNumber of clusters: 7, Silhouette score: 0.3796\nNumber of clusters: 8, Silhouette score: 0.3820\nNumber of clusters: 9, Silhouette score: 0.4362\nNumber of clusters: 10, Silhouette score: 0.3830\n\nOptimal number of clusters based on silhouette score: 9\n\n\n\n\n\n\n\n\n\nOptimal number of clusters: 9\n\n5. Performing K-means clustering...\n\n\n\n\n\n\n\n\n\nInterestingly, the varying perplexity values inputted into the t-SNE did not produce visually distinguishable plots; none of the perplexity levels portrayed significant differences in clustering or clear clustering at all. However, the data points are closer together the higher the perplexity score– but not necessarily in distinct clusters.\nPCA and t-SNE differ in their aims, methods, and outputs. As mentioned, PCA is a linear-based technique that prioritizes preserving the global structure of the data, while t-SNe is non-linear and prioritizes local structures. While t-SNE is strictly a data visualization tool, PCA can also perform dimensionality reduction on data. One clear dinstinction between the two tools with this data is the difference in the optimal number of clusters identified by performing silhouette score optimization on the PCA and t-SNE transformed data. While PCA idenfitied three clusters (which is consistent and reasonable considering the numbers identified in the three algorithms deployed below), t-SNE identified nine. This highlights imbalanced, inconsistent, and difficult data, likely due to its small sample size and complexity."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The purpose of this code is to perform exploratory data analysis (EDA) on the data through advanced visualizations and statistical testing in order to deliver insights on trends, correlations, and other informative qualities of the data. The visualizations will include a heatmap (representing a correlation matrix), line graphs (to visualize data over time), boxplots and stacked bar graphs (to compare levels of categorical variables), scatterplots (to compare numerical variables), and interactive map plots (to provide the ability to explore geospatial insights). The statistical testing methods that will be performed are the ANOVA, Kruskal-Wallis, and Chi-squared tests.\nTo start, necessary libraries and the two datasets must be loaded:\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n# temporal data\ntimedf = pd.read_csv('../../data/processed-data/complete_merge.csv')\ntimedf['Tier_2024'] = timedf['Tier_2024'].astype('category')\n\n# ML data\ndf = pd.read_csv('../../data/processed-data/MLdataset.csv')\n\n\n\n\n\n\nimport plotly.express as px\n\ntiermap = df\n\n# Make new column renaming the Tier to a more representative name for plotting\ntier_map = {\n    1.0: 'Tier 1',\n    2.0: 'Tier 2',\n    2.2: 'Tier 2 Watch List',\n    3.0: 'Tier 3'\n}\ntiermap['Tier_Label'] = tiermap['Tier_2024'].map(tier_map)\n\n# Map ISO codes to country name in dataset -- this helps choropleth identify locations\niso_codes = {\n    'Albania': 'ALB', 'Andorra': 'AND', 'Armenia': 'ARM', 'Austria': 'AUT',\n    'Azerbaijan': 'AZE', 'Belgium': 'BEL', 'Bosnia and Herzegovina': 'BIH',\n    'Bulgaria': 'BGR', 'Croatia': 'HRV', 'Cyprus': 'CYP', 'Czechia': 'CZE',\n    'Denmark': 'DNK', 'Estonia': 'EST', 'Finland': 'FIN', 'France': 'FRA',\n    'Georgia': 'GEO', 'Germany': 'DEU', 'Greece': 'GRC', 'Hungary': 'HUN',\n    'Iceland': 'ISL', 'Ireland': 'IRL', 'Italy': 'ITA', 'Latvia': 'LVA',\n    'Lithuania': 'LTU', 'Luxembourg': 'LUX', 'Malta': 'MLT', 'Moldova': 'MDA',\n    'Monaco': 'MCO', 'Montenegro': 'MNE', 'Netherlands': 'NLD',\n    'North Macedonia': 'MKD', 'Norway': 'NOR', 'Poland': 'POL', 'Portugal': 'PRT',\n    'Romania': 'ROU', 'Russian Federation': 'RUS', 'Serbia': 'SRB',\n    'Slovakia': 'SVK', 'Slovenia': 'SVN', 'Spain': 'ESP', 'Sweden': 'SWE',\n    'Switzerland': 'CHE', 'Turkiye': 'TUR', 'Ukraine': 'UKR',\n    'United Kingdom': 'GBR'\n}\n\ntiermap['ISO'] = tiermap['Country'].map(iso_codes)\n\ncolor_map = {\n    'Tier 1': '#1a9850',      \n    'Tier 2': '#fee08d',   \n    'Tier 2 Watch List': '#f46d43',  \n    'Tier 3': '#d73027'\n}\n\n# Create the choropleth map\nfig = px.choropleth(\n    tiermap,\n    locations='ISO',\n    locationmode='ISO-3',\n    color='Tier_Label',\n    scope='europe',\n    color_discrete_map=color_map,\n    category_orders={'Tier_Label': ['Tier 1', 'Tier 2', 'Tier 2 Watch List', 'Tier 3']},\n    title='Human Trafficking Tier Ratings in Europe',\n    custom_data=['Country', 'Tier_Label'] \n)\n\n\n# Add star markers to denote countries with nonpunishment policy \nmarker_data = tiermap[tiermap['Nonpunishment_policy_after2021'] == 1]\nfig.add_scattergeo(\n    locations=marker_data['ISO'],\n    locationmode='ISO-3',\n    marker=dict(size=10, symbol='star', color='black', line=dict(width=1, color='white')),\n    name='Nonpunishment Policy After 2021',\n    hoverinfo='text',\n    text=marker_data['Country']\n)\n\n# specify layout\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    legend_title_text='Tier Rating',\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=0.01\n    )\n)\n\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;%{customdata[1]}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.show()\n\n                                                \n\n\nThis map shows that there appears to be a geographical trend in the Tier placements. Countries in Northern and Western Europe, with a few exceptions, are mainly Tier 1, whereas countries in Southern Europe are exclusively Tier 2, as well as most of Eastern Europe. As for the countries with victim nonpunishment policy specified in national legislation, there appears to be no geographical trend, or relationship to Tier placements.\n\n\n\n\nimport plotly.express as px\n\n# map the country codes determined in the previous code \ntimedf['ISO'] = timedf['Country'].map(iso_codes)\n\n# interactive choropleth map\nfig = px.choropleth(\n    timedf,\n    locations='ISO',\n    locationmode='ISO-3',\n    color='Detected_victims',\n    scope='europe',\n    color_continuous_scale='Reds', # specify color palette \n    title='Changes in Human Trafficking Detections per 100 People in Europe',\n    animation_frame='Year', # adds slider feature for years\n    hover_name='Country', # hover the country name\n    labels={'Detections_per_100': 'Detected victims'},  \n    custom_data=['Country', 'Detected_victims'] \n)\n\n# specify layout\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    coloraxis_colorbar=dict(\n        title=\"Detections per 100\",\n        tickformat=\".2f\"\n    )\n)\n\n# Customize hover text\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;Detections per 100: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.show()\n\n                                                \n\n\n\n\n\n\n# same code as above, just switch to detected victims per 100 population\n\n# interactive choropleth map\nfig = px.choropleth(\n    timedf,\n    locations='ISO',\n    locationmode='ISO-3',\n    color='Detected_victims',\n    scope='europe',\n    color_continuous_scale='Reds', # specify color palette \n    title='Changes in Human Trafficking Detections per 100 People in Europe',\n    animation_frame='Year', # adds slider feature for years\n    hover_name='Country', # hover the country name\n    labels={'Detections_per_100': 'Detected victims'},  \n    custom_data=['Country', 'Detected_victims'] \n)\n\n# Layout and appearance\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    coloraxis_colorbar=dict(\n        title=\"Detections per 100\",\n        tickformat=\".2f\"\n    )\n)\n\n# Customize hover text\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;Detections per 100: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\n# Show the map\nfig.show()\n\n                                                \n\n\nThis shows that over the years, Bulgaria has consistently had a high rate of detecting trafficking victims per 100 population compared to other countries. This visualization highlights which countries with smaller populations are cracking down on detecting trafficking victims.\n\n\n\n\nimport plotly.express as px\nimport pandas as pd\n\n\ndf['ISO'] = df['Country'].map(iso_codes)\n\n# specify the color mapping for different levels of prostitution policy\ncolor_map = {\n    'prohibited': '#fa0202',    \n    'neoabolitionism': '#f57c18',   \n    'abolitionism': '#311a98',\n    'decriminalization': '#80f72a',\n    'legal': '#27981a'\n}\n\n\nfig = px.choropleth(df,\n                    locations='ISO',\n                    locationmode='ISO-3',\n                    color='prostitution_policy',\n                    scope='europe',\n                    color_discrete_map=color_map,\n                    category_orders={'prostitution_policy': ['legal', 'decriminalization', 'abolitionism', 'neoabolitionism', 'prohibited']},\n                    title='Prostitution Policy by Country in Europe',\n                    custom_data=['Country', 'prostitution_policy']\n                    )\n\n# specify layour\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    legend_title_text='Tier Rating',\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=0.01\n    )\n)\n\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;%{customdata[1]}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.show()\n\n                                                \n\n\n\n\n\n\n\n\n# drop non-numerical columns\ndropcols = ['Country', 'Subregion', 'prostitution_policy'] \nnumeric_df = df.drop(columns=dropcols)\n\n# calculate the correlation matrix\ncorrelation_matrix = numeric_df.corr()\n\n# heat map visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 16))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nKey observations: The country indicators do not have strong correlations with the human trafficking detection data– the human trafficking data only has relatively strong correlations with other detection indicators (i.e. the correlation coefficient between the sum of detected victims and the sum of repatriated victims is 0.62, which indicates that there is a relatively positive relationship between the two).\nThe highest correlation coefficients in the data between two independent variables are:\n\n0.87 between the mean criminal justice score and the mean political stability over time\n-0.80 between the mean criminal justice score and the mean Tier placement over time\n0.80 between the mean criminal justice score and Henley passport index score\n0.77 between the mean criminal justice score and the mean GDP per capita\n0.76 between the mean political stability over time and Henley passport index score\n0.72 between the mean political stability and the mean GDP per capita over time\n0.72 between the EU membership and the Henley passport index score\n-0.71 between the mean Tier placement over time and Henley passport index score\n-0.67 between the mean criminal justice score and the Tier placement in 2024\n-0.64 between the mean political stability and the mean Tier placement over time\n\nThe mean criminal justice score of a country appeared in the first five highest correlation coefficients between pairwise variables, and in six out of the ten top. This demonstrates that criminal justice score is a central quality of a country that influences and has relationships with a multitude of other variables. In terms of data that reflects government response score, the Tier placement over time appeared to be quite significantly negatively correlated (-0.80) with the mean criminal justice score, indicating that the higher of a criminal justice score, the lower the Tier placement (the more likely it is to be 1, which represents a strong government preparation to combat human trafficking). The average Tier placement over time is also moderately negatively correlated with the Henley passport index score (-0.71) (also known as the number of Visa free destinations granted by the country’s passport) and\n\n\n\nIn the code below, I subsetted to the variables that I deemed to be the most significant in the correlation matrix heatmap visualization (the variables that appeared in most of the high correlation coefficients) and visualized them in a pairplot format. This shows the specific datapoints’ relationships to one another. I also included GSI government response score and the sum of detected victims to this pairplot, because the first offers an insightful perspective into government efforts to address human trafficking and the second offers insight into results. Assessing these variables alongside the other strong indicators may reveal some important findngs. I decided to fill in the color of the data points with their Tier score in 2024.\n\nsignificants = df[['Criminal_justice_mean', 'Visa_free_destinations', 'GDP_per_capita_mean', 'Political_stability_mean', 'EU_members', 'GSI_gov_response_score', 'Detected_victims_sum', 'Tier_2024']]\n# custom palette to original orange and blue-- it started outputting the hue in shades of pink originally\ncustom_palette = sns.color_palette([\"#1f77b4\", \"#ff7f0e\", \"#aec7e8\", \"#ffbb78\"])  # blue and orange theme\nsns.pairplot(significants,hue='Tier_2024', palette=custom_palette)\n\n\n\n\n\n\n\n\nThe pairplot above reveals interesting characteristics in the data:\n\nAside from showing the relationships between numerical variables, the hue representing the Tiers also show that generally, Tier 1 is associated with higher criminal justice scoring, more visa free destinations, GDP per capita, and political stability.\n\nThe mean criminal justice scores appear to be different between countries in Tier 1 and Tier 2, as derived from the top right plot, in which the peak of the spread of Tier 1 criminal justice mean values is greater than that of Tier 2.\nGSI government response and the sum of detected victims appear to have no or very little relationship to the other indicators.* This can be inferred from the vertical line shapes of the scatter plots for in the GSI_gov_response_score column (the second from the right), which indicates that all GSI government response scores are actually very similar and do not vary over a considerable range. Thus, they are not correlated with (or influenced by) the other indicators. Furthermore, the distributions visualized in the Kernel Density Estimation plots (the non-scatterplot plots) indicates an overlap in the distribution of government response scores between Tiers, indicating that there is no significant relationship between GSI government response and Tier ranking in 2024.\nThe scatterplot shapes pertaining to the sum of detected victims all have very random and sporadic shapes, indicating that none of the variables can explain or influence the sum of detected victims.\n\n\n\n\n\n\nThis plot visualizes the total number of detected victims over time using the temporal dataset.\n\n# Sum of detected victims in each subregion per year\ndata_summary = timedf.groupby(['Year', 'Subregion'])['Detected_victims'].sum().reset_index()\n\n# line plot: \nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data_summary, x='Year', y='Detected_victims', hue='Subregion', marker='o')\nxticks = range(int(data_summary['Year'].min()), int(data_summary['Year'].max()) + 1, 2)\nplt.xticks(ticks=xticks)\nplt.title('Detected Victims Over Time by Subregion')\nplt.xlabel('Year')\nplt.ylabel('Sum of Detected Victims')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that between 2011 and 2016, human trafficking detected victims were highest in Esatern Europe. It is difficult to derive any overall trends from this visualization as the changes over time tend to be rather random. However, the number of detected victims in Southern Europe appears to be increasing rather steadily. A noticeable drop in detected victims is apparent for all regions in 2021 and 2022, likely due to the COVID-19 pandemic.\n\n\n\n\n# Sum of detected victims in each subregion per year\nper100 = timedf.groupby(['Year', 'Subregion'])['Detections_per_100'].sum().reset_index()\n\n# line plot: \nplt.figure(figsize=(12, 6))\nsns.lineplot(data=per100, x='Year', y='Detections_per_100', hue='Subregion', marker='o')\nxticks = range(int(data_summary['Year'].min()), int(data_summary['Year'].max()) + 1, 2)\nplt.xticks(ticks=xticks)\nplt.title('Detected Victims per 100 Over Time by Subregion')\nplt.xlabel('Year')\nplt.ylabel('Detected Victims per 100 People')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn contrast to the previous plot, this one shows the trends in the standardized rate of detected human trafficking victims per country per year, represented as the number of detected victims per 100 population. This plot shows that in recent years, countries in Southern Europe have surpassed all others in the number of victims they detected per 100 population.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# create figure with 5 subplots \nfig, axes = plt.subplots(3, 2, figsize=(20, 25))\naxes = axes.flatten() \n\n# create a plot for each of the five subregions\nsubregions = timedf['Subregion'].unique()\nfor idx, subregion in enumerate(subregions):\n    # filter data for the current subregion\n    subregion_data = timedf[timedf['Subregion'] == subregion]\n    \n    # plot line plot\n    ax = axes[idx] \n    for country in subregion_data['Country'].unique():\n        country_data = subregion_data[subregion_data['Country'] == country]\n        ax.plot(country_data['Year'], country_data['Detected_victims'], \n                label=country, marker='o')\n    \n    # Customize subplots\n    ax.set_title(f'Detected Victims Over Time in {subregion}', pad=20)\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Detected Victims')\n    ax.grid(True, alpha=0.3)\n    \n    # logistics\n    ax.tick_params(axis='x', rotation=45)\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Country')\n\nif len(subregions) &lt; 6:\n    fig.delaxes(axes[5])\n\n# adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nThis plot shows that interestingly, each subregion has one country that stands out compared to the others with much higher relative counts of detected victims of human trafficking. The outlier country in Southern Europe is Italy, in Western Asia it’s Turkey, in Western Europe it’s the Netherlands, in Eastern Europe it’s Romania, and in Northern Europe it’s the UK, but the data reported by the UK stops after 2016. The Netherlands appears to be the country with the highest counts of detected victims over time.\n\n\n\n\ntiersub = timedf.groupby(['Year', 'Subregion'])['Tier'].mean().reset_index()\n\n# line plot: \nplt.figure(figsize=(12, 6))\nsns.lineplot(data=tiersub, x='Year', y='Tier', hue='Subregion', marker='o')\nxticks = range(int(tiersub['Year'].min()), int(tiersub['Year'].max()) + 1, 2)\nplt.xticks(ticks=xticks)\nplt.title('Average Tier Placement per Subregion Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Tier Placement')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot visualizes the average Tier placement score per region over time. The slopes all appear to be gradually increasing, suggesting that they are falling (getting worse) in their Tier placements. Or, this may mean that the USDS is becoming more strict and evaluative in their Tier placements over time. However, this doesn’t apply to Western Asia which remains steadily at an average of Tier 2 placement over time. An important characteristic to note is the significant change in Eastern European Tier placements over the years– in 2023, it surpassed the mean of Western Asia, and the mean actually lies under a value of 2, indicating a new weight of countries on the Tier 2 Watchlist and in Tier 3.\n\n\n\n\n# Group by Year and Subregion to calculate average GDP\ndata_gdp = timedf.groupby(['Year', 'Subregion'])['GDP_per_capita'].mean().reset_index()\n\n# Line plot\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data_gdp, x='Year', y='GDP_per_capita', hue='Subregion', marker='o')\nplt.title('GDP per Capita Over Time by Subregion')\nplt.xlabel('Year')\nplt.ylabel('Average GDP per Capita')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that the the GDP per capita is substantially higher in Western and Northern Europe than the other subregions, and especially the highest in Western Europe, with a noticeable increase in recent years.\n\n\n\n\n\n\nThis plot visualizes the breakdown of countries in Tier placements in the USDS’s 2024 Trafficking in Persons report. As a reminder:\n\nTier 1 (1): countries that fully comply with the Trafficking Victim Protection Act’s (TVPA) minimum standards\nTier 2 (2): countries that do not fully meet the TVPA’s minimum standards but are making significant efforts to\nTier 2 Watchlist (2.2): countries in Tier 2 that also are either:\n\nexperiencing an increase in human trafficking prevalence that they are not proportionally responding to, OR\nfailing to provide evidence of increasing efforts\n\nTier 3 (3): countries that do not fully meet the TVPA’s minimum standards and are not making significant efforts to do so.\n\n\n# convert Tier_2024 to categorical \ndf['Tier_2024'] = df['Tier_2024'].astype('category')\n\nplt.figure(figsize=(12, 6))\nsns.countplot(data=df, x='Tier_2024') \nplt.xlabel('Tier Placements')\nplt.ylabel('Count')\nplt.title('Counts of Each Level of Tier_2024')\nplt.show()\n\n\n\n\n\n\n\n\nThis shows that the majority of countries in Europe are on Tier 2, and the second most are in Tier 1. Tier 2 Watchlist and Tier 3 both contain very few country observations in 2024.\n\n\n\nThis code creates a side-by-side boxplot to compare the total number of convicted and prosecuted traffickers in each region. This may help provide some perception on which regions are more strict about convicting traffickers once prosecuted, which may reflect how seriously they take the crime.\n\n# group by Subregion to sum convictions and prosecutions\nsub = timedf.groupby('Subregion')[['Convicted_traffickers', 'Prosecuted_traffickers']].sum().reset_index()\n\n# Melt the data for easier plotting\nsub_melted = sub.melt(id_vars='Subregion', \n                                                  value_vars=['Convicted_traffickers', 'Prosecuted_traffickers'],\n                                                  var_name='Metric', value_name='Count')\n\n# Bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(data=sub_melted, x='Subregion', y='Count', hue='Metric')\nplt.title('Convictions vs Prosecutions by Subregion')\nplt.xlabel('Subregion')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that the ratio of convicted traffickers over prosecuted traffickers is highest in Northern Europe, where there is a smaller gap between the counts of the two values. However, it is imperative to recognize the scale of the counts here– Northern Europe has far fewer cases of prosecuted traffickers. This may reflect a poorer job of law enforcement in detecting traffickrs, or that there are simply fewer traffickers in this subregion.\n\n\n\n\n# crosstab to count occurrences\ncrosstab = pd.crosstab(df['Tier_2024'], df['prostitution_policy'])\n\n# stacked bar chart\ncrosstab.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.title('Prostitution policy by Tier')\nplt.xlabel('Tier')\nplt.ylabel('Count')\nplt.legend(title='Prostitution policy')\n# set customize y axis tick intervals: \nmax_y = crosstab.values.sum(axis=1).max() \nplt.yticks(np.arange(0, max_y + 2, 2)) \nplt.show()\n\n\n\n\n\n\n\n\nThis stacked bar chart shows that few countries in Tier 1 have prohibited prostitution policy, whereas this policy constitutes a larger share in the lower Tiers.\n\n\n\n\n# crosstab to count occurrences\ncrosstab = pd.crosstab(df['Tier_2024'], df['Nonpunishment_policy_after2021'])\n\n# stacked bar chart\ncrosstab.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.title('Victim nonpunishment policy by Tier')\nplt.xlabel('Tier')\nplt.ylabel('Count')\nplt.legend(title='Victim nonpunishment policy')\nmax_y = crosstab.values.sum(axis=1).max()  \nplt.yticks(np.arange(0, max_y + 2, 2))  \nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be no noticeable difference in victim nonpunishment policy between Tiers.\n\n\n\n\n# Create a pivot table for the counts\ndf['Tier_2024'] = pd.to_numeric(df['Tier_2024'], errors='coerce')\ndf_cleaned = df.dropna(subset=['Tier_2024']).copy()\ndf_cleaned = df_cleaned[df_cleaned['Subregion'] != \"Western Asia\"].copy()\n\n# order Subregion levels\nsubregion_order = ['Northern Europe', 'Western Europe', 'Southern Europe', 'Eastern Europe']\ndf_cleaned['Subregion'] = pd.Categorical(df_cleaned['Subregion'], categories=subregion_order, ordered=True)\n\n# create pivoy table\ntier_pivot = pd.crosstab(df_cleaned['Subregion'], df_cleaned['Tier_2024'])\n\n# stacked bar chart\ntier_pivot.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='Set2')\n\nplt.title('Distribution of Tier by Subregion')\nplt.xlabel('Subregion')\nplt.ylabel('Proportion')\nplt.legend(title='Tier')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis output demonstrates not only the counts of countries in each region, but also the breakdown of Tiers. From this plot, we see that the majority of countries in Northern and Western Europe are Tier 1. There are zero Tier 1 countries in Southern Europe and all two Tier 2 Watchlist countries (Malta and Serbia) are in Southern Europe. The only Tier 3 country in Europe (Russia) is in Eastern Europe.\n\n\n\n\n\n\nThe code below outputs a visualization with four boxplots assessing the impact of various categorical vavriables on the sum of detected victims, facilitating side-by-side comparison.\n\n\nfrom scipy import stats\n\ndef boxplots(df):\n    # allow 4 subplots\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n    \n    # 1. Detected Victims sum by Tier\n    sns.boxplot(data=df, x='Tier_2024', y='Detected_victims_sum', ax=ax1)\n    ax1.set_title('Detected Victims per 100 Population by Tier')\n    ax1.set_ylabel('Number of Detected Victims')\n    \n    # 2. Detected Victims sum by Nonpunishment Policy \n    sns.boxplot(data=df, x='Nonpunishment_policy_after2021', y='Detected_victims_sum', ax=ax2)\n    ax2.set_title('Detected Victims per 100 Population by Nonpunishment Policy')\n    ax2.set_xlabel('Has Nonpunishment Policy')\n    ax2.set_ylabel('Number of Detected Victims')\n    \n    # 3. Detected Victims sum by Prostitution Policy \n    sns.boxplot(data=df, x='prostitution_policy', y='Detected_victims_sum', ax=ax3)\n    ax3.set_title('Detected Victims per 100 Population by Prostitution Policy')\n    ax3.set_xlabel('Prostitution policy')\n    ax3.set_ylabel('Number of Detected Victims')\n\n    #4. Detected Victims sum by EU membership \n    sns.boxplot(data=df, x='EU_members', y='Detected_victims_sum', ax=ax4)\n    ax4.set_title('Detected Victims per 100 Population by EU membership')\n    ax4.set_xlabel('EU Member')\n    ax4.set_ylabel('Number of Detected Victims')\n    \n\n    plt.tight_layout()\n\ncorrelation_matrix = boxplots(df)\n\n\n\n\n\n\n\n\nAmong the four categorical variables assessed, none appear to have noticeable associations with the sum of detected victims. The only trends that stand out are that the sum is much lower in the Tier 2 Watchlist compared to the other tiers. Additionally, the sum of detected victims is lower and has a lower range for the prostitution policies of prohibition and neoabolitionims. There is no discernible difference in the sum of detected victims between victim nonpunishment policy and EU membership.\n\n\n\n\n\ndef boxplots(df):\n    # allow 4 subplots\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n    \n    # 1. Mean Criminal Justice by Tier\n    sns.boxplot(data=df, x='Tier_2024', y='Criminal_justice_mean', ax=ax1)\n    ax1.set_title('Average Criminal Justice Score by Tier')\n    ax1.set_ylabel('Average Criminal Justice Score')\n    \n    # 2. Mean Criminal Justice by Nonpunishment Policy \n    sns.boxplot(data=df, x='Nonpunishment_policy_after2021', y='Criminal_justice_mean', ax=ax2)\n    ax2.set_title('Average Criminal Justice Score by Nonpunishment Policy')\n    ax2.set_xlabel('Has Nonpunishment Policy')\n    ax2.set_ylabel('Average Criminal Justice Score')\n    \n    # 3. Mean Criminal Justice by Prostitution Policy \n    sns.boxplot(data=df, x='prostitution_policy', y='Criminal_justice_mean', ax=ax3)\n    ax3.set_title('Average Criminal Justice Score by Prostitution Policy')\n    ax3.set_xlabel('Prostitution policy')\n    ax3.set_ylabel('Average Criminal Justice Score')\n\n    #4. Mean Criminal Justice by EU membership \n    sns.boxplot(data=df, x='EU_members', y='Criminal_justice_mean', ax=ax4)\n    ax4.set_title('Average Criminal Justice Score by EU membership')\n    ax4.set_xlabel('EU Member')\n    ax4.set_ylabel('Average Criminal Justice Score')\n    \n\n    plt.tight_layout()\n\ncorrelation_matrix = boxplots(df)\n\n\n\n\n\n\n\n\nThe side-by-side visualization of these boxplots conveys that criminal justice scores are higher in Tier 1, countries with neoabolitionist prostitution policy, and EU members. The criminal justice score is alarmingly low for Tier 3 countries and interestingly lower in countries with prohibited prostitution. There is not a strong apparent difference for nonpunishment policy, but the visualization suggests that the mean criminal just scores among country that do not specify victim nonpunishment policy are actually slightly higher.\n\n\n\nTo take the boxplot results of the mean criminal justice scores by Tier one step further, the statistical significance of the difference between groups will be assessed by performing the Kruskal-Wallis test Kruskal-Wallis test along with Dunn’s pairwise analysis test. These nonparametric tests were chosen because they are more robust to violations of the assumptions of ANOVA– for example, the Kruskal-Wallis test can handle very small sample sizes and varying sample sizes across groups.\n\nimport scikit_posthocs as sp\nfrom scipy.stats import kruskal\n\n# A condition for the KW test is that all groups must have at least two observations, \n# so the data must be filtered to only include these. \ndf_filtered = df.groupby('Tier_2024').filter(lambda x: len(x) &gt;= 2).copy()\n\n# drop missing values \ndf_filtered = df_filtered.dropna(subset=['Criminal_justice_mean'])\n\n# Perform Kruskal-Wallis test\ngroups = [df_filtered[df_filtered['Tier_2024'] == tier]['Criminal_justice_mean'] for tier in df_filtered['Tier_2024'].unique()]\nh_stat, p_value = kruskal(*groups)\nprint(f\"H-statistic: {h_stat}, P-value: {p_value}\")\n\n# Pairwise comparisons using Dunn’s test\npairwise_results = sp.posthoc_dunn(df_filtered, val_col='Criminal_justice_mean', group_col='Tier_2024', p_adjust='bonferroni')\nprint(pairwise_results)\n\n\nH-statistic: 17.21799939487991, P-value: 0.0006374006592237717\n          1.0       2.0  2.2       3.0\n1.0  1.000000  0.000883  1.0  0.142186\n2.0  0.000883  1.000000  1.0  1.000000\n2.2  1.000000  1.000000  1.0  1.000000\n3.0  0.142186  1.000000  1.0  1.000000\n\n\nThese results indicate that there is a statistically significant difference in the median of the mean criminal justice score between countries in Tier 1 and Tier 2 countries.\n\n\n\nTo further explore the differences visualized in the boxplots above, a t-test will be performed to statistically assess if EU members are distinguished from their counterparts when it comes to criminal justice scores.\n\nfrom scipy.stats import ttest_ind\n\nttestdf = df.dropna(subset=['Criminal_justice_mean'])\nt_stat, p_value = ttest_ind(ttestdf['EU_members'], ttestdf['Criminal_justice_mean'], equal_var=False)\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nT-statistic: -20.595011394291824, P-value: 6.5410194981716e-24\n\n\nThe extremely small p-value is well below the alpha threshold of 0.05, indicating that this data provides significant evidence to suggest a difference in the true criminal justice mean scores between EU and non-EU member countries.\n\n\n\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='Subregion', y='Criminal_justice_mean')\nplt.title('Criminal Justice Scores by Subregion')\nplt.xlabel('Subregion')\nplt.ylabel('Criminal Justice Score')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe average criminal justice scores in Western and Northern Europe appear to be consistently and considerably higher than the other regions. Eastern Europe in particular has a country that has a very low criminal justice mean score, around 20/100.\n\n\n\n\n# Convictions over prosecutions by Tier \nplt.figure(figsize=(12, 6))\nsns.boxplot(df, x='Tier_2024', y='Convictions_over_prosecutions_mean')\nplt.title('Convictions over prosecutions ratio by Tier ')\nplt.xlabel('Tier')\nplt.ylabel('Convictions over prosecutions ratio')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that, contrary to what might be expected, countries with Tier 2 Watchlist placements in 2024 actually had higher convictions over prosecutions. This signals a considerable effort by law enforcement to actually punish traffickers. However, we cannot draw any claims from this visualization because it lacks information on the sample sizes of the countries making up the different tiers.\n\n\n\n\n# GSI government response score by Tier \nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='Tier', y='GSI_gov_response_score')\nplt.title('GSI Government Response Score by Tier')\nplt.xlabel('Subregion')\nplt.ylabel('GSI Government Response Score')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that the mean government response score to human trafficking does not vary considerably among Tiers. This is thought-provoking, as Tier 1 countries are expected to have better government responses– that is the basis of their classification.\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='Tier_2024', y='Number_Repatriated_Victims_sum')\nplt.title('Sum of repatriated victims by Tier')\nplt.xlabel('Tier')\nplt.ylabel('Repatriated victims sum')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe sum of repatrated victims does not vary isgnificantly by Tier, except for in Malta and Serbia, the Tier 2 Watchlist countries. However, this may be explained by the fact that Malta at least has a very small population."
  },
  {
    "objectID": "technical-details/eda/main.html#interactive-maps",
    "href": "technical-details/eda/main.html#interactive-maps",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "import plotly.express as px\n\ntiermap = df\n\n# Make new column renaming the Tier to a more representative name for plotting\ntier_map = {\n    1.0: 'Tier 1',\n    2.0: 'Tier 2',\n    2.2: 'Tier 2 Watch List',\n    3.0: 'Tier 3'\n}\ntiermap['Tier_Label'] = tiermap['Tier_2024'].map(tier_map)\n\n# Map ISO codes to country name in dataset -- this helps choropleth identify locations\niso_codes = {\n    'Albania': 'ALB', 'Andorra': 'AND', 'Armenia': 'ARM', 'Austria': 'AUT',\n    'Azerbaijan': 'AZE', 'Belgium': 'BEL', 'Bosnia and Herzegovina': 'BIH',\n    'Bulgaria': 'BGR', 'Croatia': 'HRV', 'Cyprus': 'CYP', 'Czechia': 'CZE',\n    'Denmark': 'DNK', 'Estonia': 'EST', 'Finland': 'FIN', 'France': 'FRA',\n    'Georgia': 'GEO', 'Germany': 'DEU', 'Greece': 'GRC', 'Hungary': 'HUN',\n    'Iceland': 'ISL', 'Ireland': 'IRL', 'Italy': 'ITA', 'Latvia': 'LVA',\n    'Lithuania': 'LTU', 'Luxembourg': 'LUX', 'Malta': 'MLT', 'Moldova': 'MDA',\n    'Monaco': 'MCO', 'Montenegro': 'MNE', 'Netherlands': 'NLD',\n    'North Macedonia': 'MKD', 'Norway': 'NOR', 'Poland': 'POL', 'Portugal': 'PRT',\n    'Romania': 'ROU', 'Russian Federation': 'RUS', 'Serbia': 'SRB',\n    'Slovakia': 'SVK', 'Slovenia': 'SVN', 'Spain': 'ESP', 'Sweden': 'SWE',\n    'Switzerland': 'CHE', 'Turkiye': 'TUR', 'Ukraine': 'UKR',\n    'United Kingdom': 'GBR'\n}\n\ntiermap['ISO'] = tiermap['Country'].map(iso_codes)\n\ncolor_map = {\n    'Tier 1': '#1a9850',      \n    'Tier 2': '#fee08d',   \n    'Tier 2 Watch List': '#f46d43',  \n    'Tier 3': '#d73027'\n}\n\n# Create the choropleth map\nfig = px.choropleth(\n    tiermap,\n    locations='ISO',\n    locationmode='ISO-3',\n    color='Tier_Label',\n    scope='europe',\n    color_discrete_map=color_map,\n    category_orders={'Tier_Label': ['Tier 1', 'Tier 2', 'Tier 2 Watch List', 'Tier 3']},\n    title='Human Trafficking Tier Ratings in Europe',\n    custom_data=['Country', 'Tier_Label'] \n)\n\n\n# Add star markers to denote countries with nonpunishment policy \nmarker_data = tiermap[tiermap['Nonpunishment_policy_after2021'] == 1]\nfig.add_scattergeo(\n    locations=marker_data['ISO'],\n    locationmode='ISO-3',\n    marker=dict(size=10, symbol='star', color='black', line=dict(width=1, color='white')),\n    name='Nonpunishment Policy After 2021',\n    hoverinfo='text',\n    text=marker_data['Country']\n)\n\n# specify layout\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    legend_title_text='Tier Rating',\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=0.01\n    )\n)\n\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;%{customdata[1]}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.show()\n\n                                                \n\n\nThis map shows that there appears to be a geographical trend in the Tier placements. Countries in Northern and Western Europe, with a few exceptions, are mainly Tier 1, whereas countries in Southern Europe are exclusively Tier 2, as well as most of Eastern Europe. As for the countries with victim nonpunishment policy specified in national legislation, there appears to be no geographical trend, or relationship to Tier placements.\n\n\n\n\nimport plotly.express as px\n\n# map the country codes determined in the previous code \ntimedf['ISO'] = timedf['Country'].map(iso_codes)\n\n# interactive choropleth map\nfig = px.choropleth(\n    timedf,\n    locations='ISO',\n    locationmode='ISO-3',\n    color='Detected_victims',\n    scope='europe',\n    color_continuous_scale='Reds', # specify color palette \n    title='Changes in Human Trafficking Detections per 100 People in Europe',\n    animation_frame='Year', # adds slider feature for years\n    hover_name='Country', # hover the country name\n    labels={'Detections_per_100': 'Detected victims'},  \n    custom_data=['Country', 'Detected_victims'] \n)\n\n# specify layout\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    coloraxis_colorbar=dict(\n        title=\"Detections per 100\",\n        tickformat=\".2f\"\n    )\n)\n\n# Customize hover text\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;Detections per 100: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.show()\n\n                                                \n\n\n\n\n\n\n# same code as above, just switch to detected victims per 100 population\n\n# interactive choropleth map\nfig = px.choropleth(\n    timedf,\n    locations='ISO',\n    locationmode='ISO-3',\n    color='Detected_victims',\n    scope='europe',\n    color_continuous_scale='Reds', # specify color palette \n    title='Changes in Human Trafficking Detections per 100 People in Europe',\n    animation_frame='Year', # adds slider feature for years\n    hover_name='Country', # hover the country name\n    labels={'Detections_per_100': 'Detected victims'},  \n    custom_data=['Country', 'Detected_victims'] \n)\n\n# Layout and appearance\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    coloraxis_colorbar=dict(\n        title=\"Detections per 100\",\n        tickformat=\".2f\"\n    )\n)\n\n# Customize hover text\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;Detections per 100: %{customdata[1]:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\n# Show the map\nfig.show()\n\n                                                \n\n\nThis shows that over the years, Bulgaria has consistently had a high rate of detecting trafficking victims per 100 population compared to other countries. This visualization highlights which countries with smaller populations are cracking down on detecting trafficking victims.\n\n\n\n\nimport plotly.express as px\nimport pandas as pd\n\n\ndf['ISO'] = df['Country'].map(iso_codes)\n\n# specify the color mapping for different levels of prostitution policy\ncolor_map = {\n    'prohibited': '#fa0202',    \n    'neoabolitionism': '#f57c18',   \n    'abolitionism': '#311a98',\n    'decriminalization': '#80f72a',\n    'legal': '#27981a'\n}\n\n\nfig = px.choropleth(df,\n                    locations='ISO',\n                    locationmode='ISO-3',\n                    color='prostitution_policy',\n                    scope='europe',\n                    color_discrete_map=color_map,\n                    category_orders={'prostitution_policy': ['legal', 'decriminalization', 'abolitionism', 'neoabolitionism', 'prohibited']},\n                    title='Prostitution Policy by Country in Europe',\n                    custom_data=['Country', 'prostitution_policy']\n                    )\n\n# specify layour\nfig.update_layout(\n    title_x=0.5,\n    geo=dict(\n        showframe=False,\n        showcoastlines=True,\n        projection_type='equirectangular',\n        center=dict(lon=15, lat=50),\n        lataxis_range=[35, 70],\n        lonaxis_range=[-10, 40]\n    ),\n    width=1000,\n    height=600,\n    template='plotly_white',\n    legend_title_text='Tier Rating',\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=0.01\n    )\n)\n\nfig.update_traces(\n    hovertemplate=\"&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;%{customdata[1]}&lt;extra&gt;&lt;/extra&gt;\"\n)\n\nfig.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#heatmap-correlation-plot",
    "href": "technical-details/eda/main.html#heatmap-correlation-plot",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "# drop non-numerical columns\ndropcols = ['Country', 'Subregion', 'prostitution_policy'] \nnumeric_df = df.drop(columns=dropcols)\n\n# calculate the correlation matrix\ncorrelation_matrix = numeric_df.corr()\n\n# heat map visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 16))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nKey observations: The country indicators do not have strong correlations with the human trafficking detection data– the human trafficking data only has relatively strong correlations with other detection indicators (i.e. the correlation coefficient between the sum of detected victims and the sum of repatriated victims is 0.62, which indicates that there is a relatively positive relationship between the two).\nThe highest correlation coefficients in the data between two independent variables are:\n\n0.87 between the mean criminal justice score and the mean political stability over time\n-0.80 between the mean criminal justice score and the mean Tier placement over time\n0.80 between the mean criminal justice score and Henley passport index score\n0.77 between the mean criminal justice score and the mean GDP per capita\n0.76 between the mean political stability over time and Henley passport index score\n0.72 between the mean political stability and the mean GDP per capita over time\n0.72 between the EU membership and the Henley passport index score\n-0.71 between the mean Tier placement over time and Henley passport index score\n-0.67 between the mean criminal justice score and the Tier placement in 2024\n-0.64 between the mean political stability and the mean Tier placement over time\n\nThe mean criminal justice score of a country appeared in the first five highest correlation coefficients between pairwise variables, and in six out of the ten top. This demonstrates that criminal justice score is a central quality of a country that influences and has relationships with a multitude of other variables. In terms of data that reflects government response score, the Tier placement over time appeared to be quite significantly negatively correlated (-0.80) with the mean criminal justice score, indicating that the higher of a criminal justice score, the lower the Tier placement (the more likely it is to be 1, which represents a strong government preparation to combat human trafficking). The average Tier placement over time is also moderately negatively correlated with the Henley passport index score (-0.71) (also known as the number of Visa free destinations granted by the country’s passport) and"
  },
  {
    "objectID": "technical-details/eda/main.html#pairplot",
    "href": "technical-details/eda/main.html#pairplot",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In the code below, I subsetted to the variables that I deemed to be the most significant in the correlation matrix heatmap visualization (the variables that appeared in most of the high correlation coefficients) and visualized them in a pairplot format. This shows the specific datapoints’ relationships to one another. I also included GSI government response score and the sum of detected victims to this pairplot, because the first offers an insightful perspective into government efforts to address human trafficking and the second offers insight into results. Assessing these variables alongside the other strong indicators may reveal some important findngs. I decided to fill in the color of the data points with their Tier score in 2024.\n\nsignificants = df[['Criminal_justice_mean', 'Visa_free_destinations', 'GDP_per_capita_mean', 'Political_stability_mean', 'EU_members', 'GSI_gov_response_score', 'Detected_victims_sum', 'Tier_2024']]\n# custom palette to original orange and blue-- it started outputting the hue in shades of pink originally\ncustom_palette = sns.color_palette([\"#1f77b4\", \"#ff7f0e\", \"#aec7e8\", \"#ffbb78\"])  # blue and orange theme\nsns.pairplot(significants,hue='Tier_2024', palette=custom_palette)\n\n\n\n\n\n\n\n\nThe pairplot above reveals interesting characteristics in the data:\n\nAside from showing the relationships between numerical variables, the hue representing the Tiers also show that generally, Tier 1 is associated with higher criminal justice scoring, more visa free destinations, GDP per capita, and political stability.\n\nThe mean criminal justice scores appear to be different between countries in Tier 1 and Tier 2, as derived from the top right plot, in which the peak of the spread of Tier 1 criminal justice mean values is greater than that of Tier 2.\nGSI government response and the sum of detected victims appear to have no or very little relationship to the other indicators.* This can be inferred from the vertical line shapes of the scatter plots for in the GSI_gov_response_score column (the second from the right), which indicates that all GSI government response scores are actually very similar and do not vary over a considerable range. Thus, they are not correlated with (or influenced by) the other indicators. Furthermore, the distributions visualized in the Kernel Density Estimation plots (the non-scatterplot plots) indicates an overlap in the distribution of government response scores between Tiers, indicating that there is no significant relationship between GSI government response and Tier ranking in 2024.\nThe scatterplot shapes pertaining to the sum of detected victims all have very random and sporadic shapes, indicating that none of the variables can explain or influence the sum of detected victims."
  },
  {
    "objectID": "technical-details/eda/main.html#line-plots",
    "href": "technical-details/eda/main.html#line-plots",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This plot visualizes the total number of detected victims over time using the temporal dataset.\n\n# Sum of detected victims in each subregion per year\ndata_summary = timedf.groupby(['Year', 'Subregion'])['Detected_victims'].sum().reset_index()\n\n# line plot: \nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data_summary, x='Year', y='Detected_victims', hue='Subregion', marker='o')\nxticks = range(int(data_summary['Year'].min()), int(data_summary['Year'].max()) + 1, 2)\nplt.xticks(ticks=xticks)\nplt.title('Detected Victims Over Time by Subregion')\nplt.xlabel('Year')\nplt.ylabel('Sum of Detected Victims')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that between 2011 and 2016, human trafficking detected victims were highest in Esatern Europe. It is difficult to derive any overall trends from this visualization as the changes over time tend to be rather random. However, the number of detected victims in Southern Europe appears to be increasing rather steadily. A noticeable drop in detected victims is apparent for all regions in 2021 and 2022, likely due to the COVID-19 pandemic.\n\n\n\n\n# Sum of detected victims in each subregion per year\nper100 = timedf.groupby(['Year', 'Subregion'])['Detections_per_100'].sum().reset_index()\n\n# line plot: \nplt.figure(figsize=(12, 6))\nsns.lineplot(data=per100, x='Year', y='Detections_per_100', hue='Subregion', marker='o')\nxticks = range(int(data_summary['Year'].min()), int(data_summary['Year'].max()) + 1, 2)\nplt.xticks(ticks=xticks)\nplt.title('Detected Victims per 100 Over Time by Subregion')\nplt.xlabel('Year')\nplt.ylabel('Detected Victims per 100 People')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn contrast to the previous plot, this one shows the trends in the standardized rate of detected human trafficking victims per country per year, represented as the number of detected victims per 100 population. This plot shows that in recent years, countries in Southern Europe have surpassed all others in the number of victims they detected per 100 population.\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# create figure with 5 subplots \nfig, axes = plt.subplots(3, 2, figsize=(20, 25))\naxes = axes.flatten() \n\n# create a plot for each of the five subregions\nsubregions = timedf['Subregion'].unique()\nfor idx, subregion in enumerate(subregions):\n    # filter data for the current subregion\n    subregion_data = timedf[timedf['Subregion'] == subregion]\n    \n    # plot line plot\n    ax = axes[idx] \n    for country in subregion_data['Country'].unique():\n        country_data = subregion_data[subregion_data['Country'] == country]\n        ax.plot(country_data['Year'], country_data['Detected_victims'], \n                label=country, marker='o')\n    \n    # Customize subplots\n    ax.set_title(f'Detected Victims Over Time in {subregion}', pad=20)\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Detected Victims')\n    ax.grid(True, alpha=0.3)\n    \n    # logistics\n    ax.tick_params(axis='x', rotation=45)\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Country')\n\nif len(subregions) &lt; 6:\n    fig.delaxes(axes[5])\n\n# adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nThis plot shows that interestingly, each subregion has one country that stands out compared to the others with much higher relative counts of detected victims of human trafficking. The outlier country in Southern Europe is Italy, in Western Asia it’s Turkey, in Western Europe it’s the Netherlands, in Eastern Europe it’s Romania, and in Northern Europe it’s the UK, but the data reported by the UK stops after 2016. The Netherlands appears to be the country with the highest counts of detected victims over time.\n\n\n\n\ntiersub = timedf.groupby(['Year', 'Subregion'])['Tier'].mean().reset_index()\n\n# line plot: \nplt.figure(figsize=(12, 6))\nsns.lineplot(data=tiersub, x='Year', y='Tier', hue='Subregion', marker='o')\nxticks = range(int(tiersub['Year'].min()), int(tiersub['Year'].max()) + 1, 2)\nplt.xticks(ticks=xticks)\nplt.title('Average Tier Placement per Subregion Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Tier Placement')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot visualizes the average Tier placement score per region over time. The slopes all appear to be gradually increasing, suggesting that they are falling (getting worse) in their Tier placements. Or, this may mean that the USDS is becoming more strict and evaluative in their Tier placements over time. However, this doesn’t apply to Western Asia which remains steadily at an average of Tier 2 placement over time. An important characteristic to note is the significant change in Eastern European Tier placements over the years– in 2023, it surpassed the mean of Western Asia, and the mean actually lies under a value of 2, indicating a new weight of countries on the Tier 2 Watchlist and in Tier 3.\n\n\n\n\n# Group by Year and Subregion to calculate average GDP\ndata_gdp = timedf.groupby(['Year', 'Subregion'])['GDP_per_capita'].mean().reset_index()\n\n# Line plot\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=data_gdp, x='Year', y='GDP_per_capita', hue='Subregion', marker='o')\nplt.title('GDP per Capita Over Time by Subregion')\nplt.xlabel('Year')\nplt.ylabel('Average GDP per Capita')\nplt.legend(title='Subregion')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that the the GDP per capita is substantially higher in Western and Northern Europe than the other subregions, and especially the highest in Western Europe, with a noticeable increase in recent years."
  },
  {
    "objectID": "technical-details/eda/main.html#bar-charts",
    "href": "technical-details/eda/main.html#bar-charts",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This plot visualizes the breakdown of countries in Tier placements in the USDS’s 2024 Trafficking in Persons report. As a reminder:\n\nTier 1 (1): countries that fully comply with the Trafficking Victim Protection Act’s (TVPA) minimum standards\nTier 2 (2): countries that do not fully meet the TVPA’s minimum standards but are making significant efforts to\nTier 2 Watchlist (2.2): countries in Tier 2 that also are either:\n\nexperiencing an increase in human trafficking prevalence that they are not proportionally responding to, OR\nfailing to provide evidence of increasing efforts\n\nTier 3 (3): countries that do not fully meet the TVPA’s minimum standards and are not making significant efforts to do so.\n\n\n# convert Tier_2024 to categorical \ndf['Tier_2024'] = df['Tier_2024'].astype('category')\n\nplt.figure(figsize=(12, 6))\nsns.countplot(data=df, x='Tier_2024') \nplt.xlabel('Tier Placements')\nplt.ylabel('Count')\nplt.title('Counts of Each Level of Tier_2024')\nplt.show()\n\n\n\n\n\n\n\n\nThis shows that the majority of countries in Europe are on Tier 2, and the second most are in Tier 1. Tier 2 Watchlist and Tier 3 both contain very few country observations in 2024.\n\n\n\nThis code creates a side-by-side boxplot to compare the total number of convicted and prosecuted traffickers in each region. This may help provide some perception on which regions are more strict about convicting traffickers once prosecuted, which may reflect how seriously they take the crime.\n\n# group by Subregion to sum convictions and prosecutions\nsub = timedf.groupby('Subregion')[['Convicted_traffickers', 'Prosecuted_traffickers']].sum().reset_index()\n\n# Melt the data for easier plotting\nsub_melted = sub.melt(id_vars='Subregion', \n                                                  value_vars=['Convicted_traffickers', 'Prosecuted_traffickers'],\n                                                  var_name='Metric', value_name='Count')\n\n# Bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(data=sub_melted, x='Subregion', y='Count', hue='Metric')\nplt.title('Convictions vs Prosecutions by Subregion')\nplt.xlabel('Subregion')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that the ratio of convicted traffickers over prosecuted traffickers is highest in Northern Europe, where there is a smaller gap between the counts of the two values. However, it is imperative to recognize the scale of the counts here– Northern Europe has far fewer cases of prosecuted traffickers. This may reflect a poorer job of law enforcement in detecting traffickrs, or that there are simply fewer traffickers in this subregion.\n\n\n\n\n# crosstab to count occurrences\ncrosstab = pd.crosstab(df['Tier_2024'], df['prostitution_policy'])\n\n# stacked bar chart\ncrosstab.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.title('Prostitution policy by Tier')\nplt.xlabel('Tier')\nplt.ylabel('Count')\nplt.legend(title='Prostitution policy')\n# set customize y axis tick intervals: \nmax_y = crosstab.values.sum(axis=1).max() \nplt.yticks(np.arange(0, max_y + 2, 2)) \nplt.show()\n\n\n\n\n\n\n\n\nThis stacked bar chart shows that few countries in Tier 1 have prohibited prostitution policy, whereas this policy constitutes a larger share in the lower Tiers.\n\n\n\n\n# crosstab to count occurrences\ncrosstab = pd.crosstab(df['Tier_2024'], df['Nonpunishment_policy_after2021'])\n\n# stacked bar chart\ncrosstab.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.title('Victim nonpunishment policy by Tier')\nplt.xlabel('Tier')\nplt.ylabel('Count')\nplt.legend(title='Victim nonpunishment policy')\nmax_y = crosstab.values.sum(axis=1).max()  \nplt.yticks(np.arange(0, max_y + 2, 2))  \nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be no noticeable difference in victim nonpunishment policy between Tiers.\n\n\n\n\n# Create a pivot table for the counts\ndf['Tier_2024'] = pd.to_numeric(df['Tier_2024'], errors='coerce')\ndf_cleaned = df.dropna(subset=['Tier_2024']).copy()\ndf_cleaned = df_cleaned[df_cleaned['Subregion'] != \"Western Asia\"].copy()\n\n# order Subregion levels\nsubregion_order = ['Northern Europe', 'Western Europe', 'Southern Europe', 'Eastern Europe']\ndf_cleaned['Subregion'] = pd.Categorical(df_cleaned['Subregion'], categories=subregion_order, ordered=True)\n\n# create pivoy table\ntier_pivot = pd.crosstab(df_cleaned['Subregion'], df_cleaned['Tier_2024'])\n\n# stacked bar chart\ntier_pivot.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='Set2')\n\nplt.title('Distribution of Tier by Subregion')\nplt.xlabel('Subregion')\nplt.ylabel('Proportion')\nplt.legend(title='Tier')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis output demonstrates not only the counts of countries in each region, but also the breakdown of Tiers. From this plot, we see that the majority of countries in Northern and Western Europe are Tier 1. There are zero Tier 1 countries in Southern Europe and all two Tier 2 Watchlist countries (Malta and Serbia) are in Southern Europe. The only Tier 3 country in Europe (Russia) is in Eastern Europe."
  },
  {
    "objectID": "technical-details/eda/main.html#boxplots",
    "href": "technical-details/eda/main.html#boxplots",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The code below outputs a visualization with four boxplots assessing the impact of various categorical vavriables on the sum of detected victims, facilitating side-by-side comparison.\n\n\nfrom scipy import stats\n\ndef boxplots(df):\n    # allow 4 subplots\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n    \n    # 1. Detected Victims sum by Tier\n    sns.boxplot(data=df, x='Tier_2024', y='Detected_victims_sum', ax=ax1)\n    ax1.set_title('Detected Victims per 100 Population by Tier')\n    ax1.set_ylabel('Number of Detected Victims')\n    \n    # 2. Detected Victims sum by Nonpunishment Policy \n    sns.boxplot(data=df, x='Nonpunishment_policy_after2021', y='Detected_victims_sum', ax=ax2)\n    ax2.set_title('Detected Victims per 100 Population by Nonpunishment Policy')\n    ax2.set_xlabel('Has Nonpunishment Policy')\n    ax2.set_ylabel('Number of Detected Victims')\n    \n    # 3. Detected Victims sum by Prostitution Policy \n    sns.boxplot(data=df, x='prostitution_policy', y='Detected_victims_sum', ax=ax3)\n    ax3.set_title('Detected Victims per 100 Population by Prostitution Policy')\n    ax3.set_xlabel('Prostitution policy')\n    ax3.set_ylabel('Number of Detected Victims')\n\n    #4. Detected Victims sum by EU membership \n    sns.boxplot(data=df, x='EU_members', y='Detected_victims_sum', ax=ax4)\n    ax4.set_title('Detected Victims per 100 Population by EU membership')\n    ax4.set_xlabel('EU Member')\n    ax4.set_ylabel('Number of Detected Victims')\n    \n\n    plt.tight_layout()\n\ncorrelation_matrix = boxplots(df)\n\n\n\n\n\n\n\n\nAmong the four categorical variables assessed, none appear to have noticeable associations with the sum of detected victims. The only trends that stand out are that the sum is much lower in the Tier 2 Watchlist compared to the other tiers. Additionally, the sum of detected victims is lower and has a lower range for the prostitution policies of prohibition and neoabolitionims. There is no discernible difference in the sum of detected victims between victim nonpunishment policy and EU membership.\n\n\n\n\n\ndef boxplots(df):\n    # allow 4 subplots\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n    \n    # 1. Mean Criminal Justice by Tier\n    sns.boxplot(data=df, x='Tier_2024', y='Criminal_justice_mean', ax=ax1)\n    ax1.set_title('Average Criminal Justice Score by Tier')\n    ax1.set_ylabel('Average Criminal Justice Score')\n    \n    # 2. Mean Criminal Justice by Nonpunishment Policy \n    sns.boxplot(data=df, x='Nonpunishment_policy_after2021', y='Criminal_justice_mean', ax=ax2)\n    ax2.set_title('Average Criminal Justice Score by Nonpunishment Policy')\n    ax2.set_xlabel('Has Nonpunishment Policy')\n    ax2.set_ylabel('Average Criminal Justice Score')\n    \n    # 3. Mean Criminal Justice by Prostitution Policy \n    sns.boxplot(data=df, x='prostitution_policy', y='Criminal_justice_mean', ax=ax3)\n    ax3.set_title('Average Criminal Justice Score by Prostitution Policy')\n    ax3.set_xlabel('Prostitution policy')\n    ax3.set_ylabel('Average Criminal Justice Score')\n\n    #4. Mean Criminal Justice by EU membership \n    sns.boxplot(data=df, x='EU_members', y='Criminal_justice_mean', ax=ax4)\n    ax4.set_title('Average Criminal Justice Score by EU membership')\n    ax4.set_xlabel('EU Member')\n    ax4.set_ylabel('Average Criminal Justice Score')\n    \n\n    plt.tight_layout()\n\ncorrelation_matrix = boxplots(df)\n\n\n\n\n\n\n\n\nThe side-by-side visualization of these boxplots conveys that criminal justice scores are higher in Tier 1, countries with neoabolitionist prostitution policy, and EU members. The criminal justice score is alarmingly low for Tier 3 countries and interestingly lower in countries with prohibited prostitution. There is not a strong apparent difference for nonpunishment policy, but the visualization suggests that the mean criminal just scores among country that do not specify victim nonpunishment policy are actually slightly higher.\n\n\n\nTo take the boxplot results of the mean criminal justice scores by Tier one step further, the statistical significance of the difference between groups will be assessed by performing the Kruskal-Wallis test Kruskal-Wallis test along with Dunn’s pairwise analysis test. These nonparametric tests were chosen because they are more robust to violations of the assumptions of ANOVA– for example, the Kruskal-Wallis test can handle very small sample sizes and varying sample sizes across groups.\n\nimport scikit_posthocs as sp\nfrom scipy.stats import kruskal\n\n# A condition for the KW test is that all groups must have at least two observations, \n# so the data must be filtered to only include these. \ndf_filtered = df.groupby('Tier_2024').filter(lambda x: len(x) &gt;= 2).copy()\n\n# drop missing values \ndf_filtered = df_filtered.dropna(subset=['Criminal_justice_mean'])\n\n# Perform Kruskal-Wallis test\ngroups = [df_filtered[df_filtered['Tier_2024'] == tier]['Criminal_justice_mean'] for tier in df_filtered['Tier_2024'].unique()]\nh_stat, p_value = kruskal(*groups)\nprint(f\"H-statistic: {h_stat}, P-value: {p_value}\")\n\n# Pairwise comparisons using Dunn’s test\npairwise_results = sp.posthoc_dunn(df_filtered, val_col='Criminal_justice_mean', group_col='Tier_2024', p_adjust='bonferroni')\nprint(pairwise_results)\n\n\nH-statistic: 17.21799939487991, P-value: 0.0006374006592237717\n          1.0       2.0  2.2       3.0\n1.0  1.000000  0.000883  1.0  0.142186\n2.0  0.000883  1.000000  1.0  1.000000\n2.2  1.000000  1.000000  1.0  1.000000\n3.0  0.142186  1.000000  1.0  1.000000\n\n\nThese results indicate that there is a statistically significant difference in the median of the mean criminal justice score between countries in Tier 1 and Tier 2 countries.\n\n\n\nTo further explore the differences visualized in the boxplots above, a t-test will be performed to statistically assess if EU members are distinguished from their counterparts when it comes to criminal justice scores.\n\nfrom scipy.stats import ttest_ind\n\nttestdf = df.dropna(subset=['Criminal_justice_mean'])\nt_stat, p_value = ttest_ind(ttestdf['EU_members'], ttestdf['Criminal_justice_mean'], equal_var=False)\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nT-statistic: -20.595011394291824, P-value: 6.5410194981716e-24\n\n\nThe extremely small p-value is well below the alpha threshold of 0.05, indicating that this data provides significant evidence to suggest a difference in the true criminal justice mean scores between EU and non-EU member countries.\n\n\n\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='Subregion', y='Criminal_justice_mean')\nplt.title('Criminal Justice Scores by Subregion')\nplt.xlabel('Subregion')\nplt.ylabel('Criminal Justice Score')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe average criminal justice scores in Western and Northern Europe appear to be consistently and considerably higher than the other regions. Eastern Europe in particular has a country that has a very low criminal justice mean score, around 20/100.\n\n\n\n\n# Convictions over prosecutions by Tier \nplt.figure(figsize=(12, 6))\nsns.boxplot(df, x='Tier_2024', y='Convictions_over_prosecutions_mean')\nplt.title('Convictions over prosecutions ratio by Tier ')\nplt.xlabel('Tier')\nplt.ylabel('Convictions over prosecutions ratio')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that, contrary to what might be expected, countries with Tier 2 Watchlist placements in 2024 actually had higher convictions over prosecutions. This signals a considerable effort by law enforcement to actually punish traffickers. However, we cannot draw any claims from this visualization because it lacks information on the sample sizes of the countries making up the different tiers.\n\n\n\n\n# GSI government response score by Tier \nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='Tier', y='GSI_gov_response_score')\nplt.title('GSI Government Response Score by Tier')\nplt.xlabel('Subregion')\nplt.ylabel('GSI Government Response Score')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows that the mean government response score to human trafficking does not vary considerably among Tiers. This is thought-provoking, as Tier 1 countries are expected to have better government responses– that is the basis of their classification.\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x='Tier_2024', y='Number_Repatriated_Victims_sum')\nplt.title('Sum of repatriated victims by Tier')\nplt.xlabel('Tier')\nplt.ylabel('Repatriated victims sum')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe sum of repatrated victims does not vary isgnificantly by Tier, except for in Malta and Serbia, the Tier 2 Watchlist countries. However, this may be explained by the fact that Malta at least has a very small population."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "In this document, I will combine the individual datasets retained in the data collection phase. All of the data comes in tabular format (in either Excel or CSV files), but they have inconsistencies in various qualities, such as the country name presentation (some sources use a country’s full, official title– for example, “United Kingdom of Great Britain and Northern Ireland” versus “United Kingdom”). The goal of this data cleaning code is to standardize data to a consistent format in order to merge all of the indicators into one dataset.\nThis process will ultimately produce two datasets: one that includes temporal aspects of the data (so it includes data over various years for each country), and one that aggregates the temporal data to representative metrics such as the slope and mean so that the time series aspect is mitigated and the data can be used in machine learning models. More information on this process is described in the “ML dataset” section below."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#tier-placements-over-time-data",
    "href": "technical-details/data-cleaning/main.html#tier-placements-over-time-data",
    "title": "Data Cleaning",
    "section": "Tier placements over time data",
    "text": "Tier placements over time data\n\n\ntiers = pd.read_csv('../../data/raw-data/TIP_Tiers_overyears.csv')\n\n# filter to only keep rows where Location has \"Europe\" in the entry value\ntiers = tiers[tiers['Location'].str.contains('Europe', na=False)].copy()\n\n# clean column names-- remove brackets and content within brackets\ntiers.columns = tiers.columns.str.replace(r\"\\[.*\\]\", \"\", regex=True).str.strip()\n\n# drop unnecessary columns\ntiers = tiers.drop(columns=['Location', 'Main article'])\n\n# clean country names using the mapping from the previous step-- this ensures that this data matches the existing df\ncountry_name_mapping = {\n    \"Türkiye\": \"Turkiye\",\n    \"Turkey\": \"Turkiye\",\n    \"Slovak Republic\": \"Slovakia\",\n    \"Republic of Moldova\": \"Moldova\",\n    \"Kyrgyz Republic\": \"Kyrgyzstan\",\n    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n    \"Czech Republic\": \"Czechia\",\n    \"Russia\": \"Russian Federation\",\n}\ntiers['Country'] = tiers['Country'].replace(country_name_mapping)\n\n# melt the data into long format (columns: Country, Year, Tier Placement)\ntier_long = tiers.melt(id_vars=['Country'], \n                         var_name='Year', \n                         value_name='Tier')\n\n# replace Tier '2w' to 2.2 \ntier_long['Tier'] = tier_long['Tier'].replace('2w', '2.2')\n\n# convert 'Tier Placement' and 'Year' to numeric \ntier_long['Tier'] = pd.to_numeric(tier_long['Tier'], errors='coerce')\ntier_long['Year'] = pd.to_numeric(tier_long['Year'], errors='coerce')\n\nprint(tier_long.head())\n# MERGE with dataset\ndf = df.merge(tier_long, on=[\"Country\", \"Year\"], how=\"outer\")\n\n                  Country  Year  Tier\n0                 Albania  2011   2.0\n1                 Austria  2011   1.0\n2                 Belarus  2011   2.0\n3                 Belgium  2011   1.0\n4  Bosnia and Herzegovina  2011   1.0\n\n\nNext, I will create a dataset that has “static” variables. These are variables for which there was only data available for the status of a country for the most recent time, and not over many years. These variables have more to do with government policy.\n\nHenley passport data\n\n# load dataset with Henley Passport Index Data\npassport = pd.read_csv('../../data/raw-data/henley_passport_index.csv')\n\n# clean the country names using some additions to the country_name_mapping dictionary defined earlier: \ncountry_name_mapping = {\n    \"Türkiye\": \"Turkiye\",\n    \"Turkey\": \"Turkiye\",\n    \"Slovak Republic\": \"Slovakia\",\n    \"Republic of Moldova\": \"Moldova\",\n    \"Kyrgyz Republic\": \"Kyrgyzstan\",\n    \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n    \"Czech Republic\": \"Czechia\",\n    \"Russia\": \"Russian Federation\",\n}\n\npassport['Country'] = passport['Country'].replace(country_name_mapping)\n\n# clean the Country column by subsetting to the countries in df\nEU_centralasia = df['Country'].unique()\npassport = passport[passport['Country'].isin(EU_centralasia)]\nprint(passport.head())\nprint(passport['Country'].unique())\n\n       Country  Visa_free_destinations\n1      Albania                     123\n3      Andorra                     171\n7      Armenia                      68\n9      Austria                     191\n10  Azerbaijan                      71\n['Albania' 'Andorra' 'Armenia' 'Austria' 'Azerbaijan' 'Belarus' 'Belgium'\n 'Bosnia and Herzegovina' 'Bulgaria' 'Croatia' 'Cyprus' 'Czechia'\n 'Denmark' 'Estonia' 'Finland' 'France' 'Georgia' 'Germany' 'Greece'\n 'Hungary' 'Iceland' 'Ireland' 'Italy' 'Kazakhstan' 'Kosovo' 'Kyrgyzstan'\n 'Latvia' 'Liechtenstein' 'Lithuania' 'Luxembourg' 'Malta' 'Moldova'\n 'Montenegro' 'Netherlands' 'North Macedonia' 'Norway' 'Poland' 'Portugal'\n 'Romania' 'Russian Federation' 'Serbia' 'Slovakia' 'Slovenia' 'Spain'\n 'Sweden' 'Switzerland' 'Tajikistan' 'Turkiye' 'Turkmenistan' 'Ukraine'\n 'United Arab Emirates' 'United Kingdom' 'Uzbekistan']"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#policy-data",
    "href": "technical-details/data-cleaning/main.html#policy-data",
    "title": "Data Cleaning",
    "section": "Policy data",
    "text": "Policy data\n\n# load the manually compiled policy dataset\npolicy = pd.read_excel('../../data/raw-data/EU_policy.xlsx')\n\n# clean the data:\n# Replace \"X\" with 1\npolicy = policy.replace(\"X\", 1)\n# Replace NaN with 0\npolicy = policy.fillna(0)\n# clean country names \npolicy['Country'] = policy['Country'].replace(country_name_mapping)\n\n# Define a mapping for the Tier columns\ntier_mapping = {\n    \"Tier_1\": 1,\n    \"Tier_2\": 2,\n    \"Tier_2_watchlist\": 2.2,\n    \"Tier_3\": 3\n}\n\n# Melt the DataFrame for the Tier columns\ntier_columns = [\"Tier_1\", \"Tier_2\", \"Tier_2_watchlist\", \"Tier_3\"]\nmelted = policy.melt(id_vars=[\"Country\"], value_vars=tier_columns, \n                     var_name=\"Tier_Type\", value_name=\"Tier_Value\")\n\n# Map the Tier_Type to numerical values and filter rows where Tier_Value is 1\nmelted['Tier_2024'] = melted['Tier_Type'].map(tier_mapping)\nmelted = melted[melted['Tier_Value'] == 1]\n\n# Drop unnecessary columns and merge back if needed\nmelted = melted[['Country', 'Tier_2024']]\n\npolicy = policy.drop(columns=[\"Tier_1\", \"Tier_2\", \"Tier_2_watchlist\", \"Tier_3\"])\n\n# Merge back to the original DataFrame if required\npolicy = policy.merge(melted, on=\"Country\", how=\"left\")\n\nprint(policy.head())\nprint(policy.columns)\n\n      Country  Nonpunishment_policy_before2021  \\\n0     Albania                              0.0   \n1     Andorra                              0.0   \n2     Armenia                              1.0   \n3     Austria                              0.0   \n4  Azerbaijan                              1.0   \n\n   Nonpunishment_policy_after2021  Limited_nonpunishment_policy  \\\n0                             1.0                           0.0   \n1                             0.0                           0.0   \n2                             1.0                           1.0   \n3                             0.0                           0.0   \n4                             1.0                           0.0   \n\n   Prostitution_abolitionism  Prostitution_neoabolitionism  \\\n0                        0.0                           0.0   \n1                        0.0                           0.0   \n2                        0.0                           0.0   \n3                        0.0                           0.0   \n4                        0.0                           0.0   \n\n   Prostitution_decriminalization  Prostitution_legal  \\\n0                             0.0                 0.0   \n1                             0.0                 0.0   \n2                             0.0                 0.0   \n3                             0.0                 1.0   \n4                             0.0                 0.0   \n\n   Prostitution_prohibited  Post_soviet_states  EU_members  Tier_2024  \n0                      1.0                 1.0         0.0        2.0  \n1                      0.0                 0.0         0.0        NaN  \n2                      1.0                 1.0         0.0        2.0  \n3                      0.0                 0.0         1.0        1.0  \n4                      1.0                 1.0         0.0        2.0  \nIndex(['Country', 'Nonpunishment_policy_before2021',\n       'Nonpunishment_policy_after2021', 'Limited_nonpunishment_policy',\n       'Prostitution_abolitionism', 'Prostitution_neoabolitionism',\n       'Prostitution_decriminalization', 'Prostitution_legal',\n       'Prostitution_prohibited', 'Post_soviet_states', 'EU_members',\n       'Tier_2024'],\n      dtype='object')\n\n\n/var/folders/0s/3s0t3s4d31d4xtm_jt4pfhpr0000gn/T/ipykernel_2475/3635385757.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  policy = policy.replace(\"X\", 1)\n\n\n\npolicy['Tier_2024'].unique()\n\narray([2. , nan, 1. , 3. , 2.2])\n\n\n\n# isolate unique country names from both datasets\npassport_countries = passport['Country'].unique()\npolicy_countries = policy['Country'].unique()\n\n# Show countries in totals but not in wb\nmissing_in_passport = set(policy_countries) - set(passport_countries)\n# Show countries in wb but not in totals\nmissing_in_policy = set(passport_countries) - set(policy_countries)\n\nprint(\"Countries in policy data but missing in passport data:\\n\", missing_in_passport)\nprint(\"\\nCountries in passport index data but not in policy data:\\n\", missing_in_policy)\n\nCountries in policy data but missing in passport data:\n {'Monaco'}\n\nCountries in passport index data but not in policy data:\n {'Kazakhstan', 'Turkmenistan', 'Tajikistan', 'Liechtenstein', 'Kyrgyzstan', 'United Arab Emirates', 'Uzbekistan'}\n\n\nFrom this, I see that the passport data is missing an entry for Turkiye, Czech Rebpulic, Belarus, Kosovo and Russia (likely because of a different notation), so I will go back to the previous cleaning steps to include those. To do so, I will manually look through the csv outputted in the data collection phase.\nThis shows the inconsistencies in country notation between the two datasets. Similar to the country name mapping performed previously, I will manually construct a dictionary given these differences and apply them to both datasets to ensure matching.\n\nGSI government response score data from Walk Free\nThe first dataset that is loaded in the cell below is from the Walk Free Foundation, an international non-profit human rights group dedicated to research and spreading awareness in the pursuit of eradicating modern slavery. It has a comprehensive dataset for the year 2023 titled “The Global Slavery Index”. One of the indicators for which it collects data is what they call the “government response score”. This is calculated through a complex framework encapsulating a variety of activities performed by a government to address modern slavery, ranging from efforts for ensuring appropriate identification and protection of victims, to deploying the necessary personnel and resources to adequately address modern slavery, to having a strong judicial system to trained to tackle modern slavery cases. Walk Free experts calculate this metric from extensive policy research. It is important to note that this is not an official, validated, government-verified source.\n\n# load Walk Free's Global Slavery Index dataset\n# extract only the 5th sheet of the excel file, which contains data on the government response score\nwalkfree = pd.read_excel('../../data/raw-data/2023-Global-Slavery-Index-Data.xlsx', sheet_name=4, skiprows=2)\n\n# subset to countries in Europe and Central Asia \nwalkfree = walkfree[walkfree[\"Region\"] == \"Europe and Central Asia\"]\n\n# subset to only include important columns -- Country and the Total (which refers to the total number of appropriate anti-trafficking actions taken by government)\nwalkfree = walkfree[[\"Country\", \"Total (n)\"]]\n\n# clean the Country column with the dictionary previously defined\nwalkfree['Country'] = walkfree['Country'].replace(country_name_mapping)\n\n# rename the column to the appropriate, identifiable indicator \nwalkfree = walkfree.rename(columns={\"Total (n)\": \"GSI_gov_response_score\"})\n\nprint(walkfree.head())\n\n       Country  GSI_gov_response_score\n0      Albania                    48.0\n5      Armenia                    42.0\n7      Austria                    48.0\n8   Azerbaijan                    46.0\n13     Belarus                    37.0\n\n\nNow, I will merge the henley passport index, policy data, and walk free data into one dataset:\n\nstatic_data = policy.merge(passport, on=\"Country\", how=\"left\")\nprint(static_data.head())\nstatic_data = static_data.merge(walkfree, on=\"Country\", how=\"left\")\n\n      Country  Nonpunishment_policy_before2021  \\\n0     Albania                              0.0   \n1     Andorra                              0.0   \n2     Armenia                              1.0   \n3     Austria                              0.0   \n4  Azerbaijan                              1.0   \n\n   Nonpunishment_policy_after2021  Limited_nonpunishment_policy  \\\n0                             1.0                           0.0   \n1                             0.0                           0.0   \n2                             1.0                           1.0   \n3                             0.0                           0.0   \n4                             1.0                           0.0   \n\n   Prostitution_abolitionism  Prostitution_neoabolitionism  \\\n0                        0.0                           0.0   \n1                        0.0                           0.0   \n2                        0.0                           0.0   \n3                        0.0                           0.0   \n4                        0.0                           0.0   \n\n   Prostitution_decriminalization  Prostitution_legal  \\\n0                             0.0                 0.0   \n1                             0.0                 0.0   \n2                             0.0                 0.0   \n3                             0.0                 1.0   \n4                             0.0                 0.0   \n\n   Prostitution_prohibited  Post_soviet_states  EU_members  Tier_2024  \\\n0                      1.0                 1.0         0.0        2.0   \n1                      0.0                 0.0         0.0        NaN   \n2                      1.0                 1.0         0.0        2.0   \n3                      0.0                 0.0         1.0        1.0   \n4                      1.0                 1.0         0.0        2.0   \n\n   Visa_free_destinations  \n0                   123.0  \n1                   171.0  \n2                    68.0  \n3                   191.0  \n4                    71.0  \n\n\n\n\nMerge all into comprehensive dataset\n\nfinal_merge = df.merge(static_data, on=\"Country\", how=\"inner\")\nfinal_merge.head()\n\n\n\n\n\n\n\n\nCountry\nSubregion\nYear\nDetected_victims\nConvicted_traffickers\nProsecuted_traffickers\nconvictions_over_prosecutions\nNumber_Repatriated_Victims\nCriminal_justice\nGDP_per_capita\n...\nProstitution_abolitionism\nProstitution_neoabolitionism\nProstitution_decriminalization\nProstitution_legal\nProstitution_prohibited\nPost_soviet_states\nEU_members\nTier_2024\nVisa_free_destinations\nGSI_gov_response_score\n\n\n\n\n0\nAlbania\nSouthern Europe\n2010\n97.0\n17.0\n39.0\n0.435897\nNaN\n42.654030\n4094.349686\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n2.0\n123.0\n48.0\n\n\n1\nAlbania\nSouthern Europe\n2011\n84.0\n15.0\n28.0\n0.535714\nNaN\n41.314552\n4437.141161\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n2.0\n123.0\n48.0\n\n\n2\nAlbania\nSouthern Europe\n2012\n92.0\n5.0\n30.0\n0.166667\nNaN\n39.906105\n4247.631343\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n2.0\n123.0\n48.0\n\n\n3\nAlbania\nSouthern Europe\n2013\n95.0\n5.0\n42.0\n0.119048\nNaN\n38.967136\n4413.063383\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n2.0\n123.0\n48.0\n\n\n4\nAlbania\nSouthern Europe\n2014\n125.0\nNaN\nNaN\nNaN\n5.0\n44.230770\n4578.633208\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n2.0\n123.0\n48.0\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow, I have noticed that to perform EDA more easily, it would be better to melt the prostitution policy data into a single categorical column, instead of multiple binary columns with inputs of 0 or 1. The code below performs this transformation:\n\n# create the prostitution policy mapping\nprostitution_map = {}\n\n# identify all of the prostitution columns (representing the different levels of policy)\nprostitution_cols = [col for col in final_merge.columns if col.startswith('Prostitution_')]\n\n# ror each row, find which prostitution policy is marked as 1\nfor idx, row in final_merge.iterrows():\n    for col in prostitution_cols:\n        if row[col] == 1:\n            # Get the policy name from after the underscore\n            policy = col.split('_')[1].lower()\n            # Create a key of country and year\n            key = (row['Country'], row['Year'])\n            prostitution_map[key] = policy\n\n# add this new column to the merged dataset and drop the original binary prostitution policy columns\nfinal_merge['prostitution_policy'] = final_merge.apply(lambda row: prostitution_map.get((row['Country'], row['Year']), None), axis=1)\nfinal_merge = final_merge.drop(columns=prostitution_cols)\n\n# view the output\nprint(final_merge[['Country', 'Year', 'prostitution_policy']].head())\n\n   Country  Year prostitution_policy\n0  Albania  2010          prohibited\n1  Albania  2011          prohibited\n2  Albania  2012          prohibited\n3  Albania  2013          prohibited\n4  Albania  2014          prohibited\n\n\nThe output looks good! This dataframe will now be saved to a CSV for use in the EDA section of the project.\n\nfinal_merge.to_csv('../../data/processed-data/complete_merge.csv', index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#create-ml-dataset",
    "href": "technical-details/data-cleaning/main.html#create-ml-dataset",
    "title": "Data Cleaning",
    "section": "Create ML Dataset",
    "text": "Create ML Dataset\nThe dataset compiled in the previous steps include observations over time. However, time series data cannot be used in general machine learning models, because the autocorrelation of the observations violates the I.I.D. assumption of machine learning data, which assumes that data is independently and identically distributed. While this temporal data will be useful for data visualization and exploring trends over time, the temporal qualities need to be addressed in order to use this data in supervised and unsupervised learning models. There are various ways to manipulate time-series data into data for machine learning. For this project, the time series data will be handled by aggregating the temporal data into single point estimates for each country over all years. The data over the years will either be boiled down to the slope (rate of change) of the data over the years using scipy.stat’s linregress tool, the mean over the years, or the sum over the years. The aggregate metrics for each specific variable were chosen with particular consideration of the meaning of the variable. For example, the number of detected victims was aggregated to its sum because the total number may provide more insight, since the number of detected victims varies significantly over years and this may distort the mean. This makes the data appropriate to use in ML models where Time Series data violates the I.I.D assumptions.\nA function to calculate the slope using linregress was created and the aggregation metrics were applied in a for loop.\n\nimport numpy as np\nfrom scipy.stats import linregress\n\ndf = final_merge\n\ndef calculate_slope(group, x_col, y_col):\n    # x_col and y_col identify the indices; for each column specified in the foor loop, each constant y_col value will match with each unique x_col value (representing the country and year)\n    # drop any missing pairs so that the slope does not produce an error \n    filtered_group = group.dropna(subset=[x_col, y_col])\n    \n    if len(filtered_group) &gt; 1:  # calculating slope requires at least two data points\n        x = filtered_group[x_col]\n        y = filtered_group[y_col]\n        return linregress(x, y).slope\n    return np.nan  # return NA if not enough data points for a slope calculation\n\n# Aggregate metrics\naggregated_data = []\n\n# Group by country:\nfor country, group in df.groupby('Country'):\n    group = group.sort_values('Year')  # sort by year for slope calculations\n    \n    # calculate aggregated values\n    data = {\n        'Country': country,\n        'Detected_victims_sum': group['Detected_victims'].sum(skipna=True), # skipna=True ensures that NAs don't cause error in the output \n        'Detected_victims_slope': calculate_slope(group, 'Year', 'Detected_victims'),\n        'Convicted_traffickers_sum': group['Convicted_traffickers'].sum(skipna=True),\n        'Convicted_traffickers_slope': calculate_slope(group, 'Year', 'Convicted_traffickers'),\n        'Prosecuted_traffickers_sum': group['Prosecuted_traffickers'].sum(skipna=True),\n        'Prosecuted_traffickers_slope': calculate_slope(group, 'Year', 'Prosecuted_traffickers'),\n        'Convictions_over_prosecutions_mean': group['convictions_over_prosecutions'].mean(skipna=True),\n        'Number_Repatriated_Victims_sum': group['Number_Repatriated_Victims'].sum(skipna=True),\n        'Tier_mean': group['Tier'].mean(skipna=True),\n        'Tier_slope': calculate_slope(group, 'Year', 'Tier'),\n        'Criminal_justice_mean': group['Criminal_justice'].mean(skipna=True),\n        'Criminal_justice_slope': calculate_slope(group, 'Year', 'Criminal_justice'),\n        'GDP_per_capita_mean': group['GDP_per_capita'].mean(skipna=True),\n        'GDP_per_capita_slope': calculate_slope(group, 'Year', 'GDP_per_capita'),\n        'Political_stability_mean': group['Political_stability'].mean(skipna=True),\n        'Political_stability_slope': calculate_slope(group, 'Year', 'Political_stability'),\n        'Population_mean': group['Population'].mean(skipna=True),\n        'Refugee_population_mean': group['Refugee_population'].mean(skipna=True),\n        'Refugee_population_slope': calculate_slope(group, 'Year', 'Refugee_population'),\n        'Unemployment_rate_mean': group['Unemployment_rate'].mean(skipna=True),\n        'Unemployment_rate_slope': calculate_slope(group, 'Year', 'Unemployment_rate'),\n        'Detections_per_100_mean': group['Detections_per_100'].mean(skipna=True)\n    }\n    aggregated_data.append(data)\n\n# additional column:\n# count the number of years each country has reported detected victims of human trafficking \nreported_years = df.groupby('Country')['Detected_victims'].count().reset_index()\nreported_years.rename(columns={'Detected_victims': 'Reported_Years_Detected_Victims'}, inplace=True)\n# a country with a high value in this column suggests 1.) a high prevalence of human trafficking in the country or 2.) \n# a dedicated government effort to detecting victims\n\n# store as dataframe\naggregated_df = pd.DataFrame(aggregated_data)\n\n# merge the number of reported years\naggregated_df = aggregated_df.merge(reported_years, on=\"Country\", how=\"left\")\n\naggregated_df.head()\n\n\n\n\n\n\n\n\nCountry\nDetected_victims_sum\nDetected_victims_slope\nConvicted_traffickers_sum\nConvicted_traffickers_slope\nProsecuted_traffickers_sum\nProsecuted_traffickers_slope\nConvictions_over_prosecutions_mean\nNumber_Repatriated_Victims_sum\nTier_mean\n...\nGDP_per_capita_slope\nPolitical_stability_mean\nPolitical_stability_slope\nPopulation_mean\nRefugee_population_mean\nRefugee_population_slope\nUnemployment_rate_mean\nUnemployment_rate_slope\nDetections_per_100_mean\nReported_Years_Detected_Victims\n\n\n\n\n0\nAlbania\n1235.0\n2.353147\n42.0\n-4.600000\n349.0\n1.361111\n0.314332\n92.0\n2.016667\n...\n159.811093\n51.233780\n1.355719\n2.875263e+06\n107.083333\n3.884615\n14.4861\n-0.198939\n0.003584\n12\n\n\n1\nAndorra\n25.0\n0.000000\n10.0\n0.000000\n43.0\n-2.153846\n0.575758\n0.0\nNaN\n...\n-282.908853\n98.107819\n0.477088\n7.638540e+04\nNaN\nNaN\nNaN\nNaN\n0.006550\n5\n\n\n2\nArmenia\n177.0\n-0.206294\n75.0\n-0.248918\n83.0\n-1.416667\n0.833808\n0.0\nNaN\n...\n142.277720\n34.566121\n-2.971136\n2.869258e+06\n22847.083333\n4956.115385\n15.0360\n-1.061524\n0.000514\n12\n\n\n3\nAustria\n1344.0\n25.339827\n134.0\n-0.843034\n1194.0\n16.659612\n0.213883\n96.0\n1.000000\n...\n121.474676\n86.073943\n-2.083460\n8.663381e+06\n89785.700000\n11512.599567\n5.2954\n-0.005429\n0.001531\n10\n\n\n4\nAzerbaijan\n792.0\n6.300699\n231.0\n0.956522\n177.0\n-0.392857\n0.762087\n0.0\nNaN\n...\n-275.485406\n24.015193\n-1.409298\n9.660932e+06\n1403.500000\n-28.510490\n5.3550\n0.068252\n0.000676\n12\n\n\n\n\n5 rows × 24 columns\n\n\n\nNext, I add the static variables to this temporally aggregated dataframe by extracted the first observation of each static indicator for each country (so a single value is extracted) to merge with the aggregated dataframe.\n\nstatic_vars = final_merge[['Country', 'Subregion', 'Nonpunishment_policy_before2021',\n       'Nonpunishment_policy_after2021', 'Limited_nonpunishment_policy',\n       'Post_soviet_states', 'EU_members', 'Tier_2024', 'Visa_free_destinations',\n       'GSI_gov_response_score', 'prostitution_policy']]\n\nstatic_vars = static_vars.drop_duplicates(subset='Country', keep='first')\nprint(static_vars)\n\n                    Country        Subregion  Nonpunishment_policy_before2021  \\\n0                   Albania  Southern Europe                              0.0   \n13                  Andorra  Southern Europe                              0.0   \n18                  Armenia     Western Asia                              1.0   \n30                  Austria   Western Europe                              0.0   \n43               Azerbaijan     Western Asia                              1.0   \n55                  Belarus              NaN                              1.0   \n67                  Belgium              NaN                              0.0   \n79   Bosnia and Herzegovina  Southern Europe                              0.0   \n92                 Bulgaria   Eastern Europe                              0.0   \n105                 Croatia  Southern Europe                              0.0   \n118                  Cyprus     Western Asia                              1.0   \n127                 Czechia   Eastern Europe                              0.0   \n140                 Denmark  Northern Europe                              0.0   \n153                 Estonia  Northern Europe                              0.0   \n166                 Finland  Northern Europe                              1.0   \n179                  France              NaN                              0.0   \n191                 Georgia     Western Asia                              1.0   \n202                 Germany   Western Europe                              1.0   \n215                  Greece  Southern Europe                              1.0   \n228                 Hungary   Eastern Europe                              0.0   \n241                 Iceland              NaN                              0.0   \n253                 Ireland  Northern Europe                              0.0   \n266                   Italy  Southern Europe                              0.0   \n279                  Kosovo              NaN                              0.0   \n291                  Latvia  Northern Europe                              0.0   \n304               Lithuania  Northern Europe                              1.0   \n317              Luxembourg              NaN                              1.0   \n329                   Malta  Southern Europe                              1.0   \n341                 Moldova   Eastern Europe                              0.0   \n354              Montenegro  Southern Europe                              0.0   \n367             Netherlands   Western Europe                              0.0   \n380         North Macedonia  Southern Europe                              0.0   \n393                  Norway  Northern Europe                              0.0   \n406                  Poland              NaN                              0.0   \n418                Portugal  Southern Europe                              0.0   \n431                 Romania              NaN                              1.0   \n443      Russian Federation   Eastern Europe                              0.0   \n448                  Serbia  Southern Europe                              0.0   \n461                Slovakia   Eastern Europe                              0.0   \n474                Slovenia  Southern Europe                              0.0   \n487                   Spain              NaN                              1.0   \n499                  Sweden  Northern Europe                              0.0   \n512             Switzerland   Western Europe                              0.0   \n525                 Turkiye     Western Asia                              0.0   \n538                 Ukraine   Eastern Europe                              0.0   \n551          United Kingdom  Northern Europe                              0.0   \n\n     Nonpunishment_policy_after2021  Limited_nonpunishment_policy  \\\n0                               1.0                           0.0   \n13                              0.0                           0.0   \n18                              1.0                           1.0   \n30                              0.0                           0.0   \n43                              1.0                           0.0   \n55                              1.0                           1.0   \n67                              0.0                           0.0   \n79                              1.0                           0.0   \n92                              1.0                           0.0   \n105                             0.0                           0.0   \n118                             1.0                           0.0   \n127                             0.0                           0.0   \n140                             0.0                           0.0   \n153                             0.0                           0.0   \n166                             1.0                           1.0   \n179                             0.0                           0.0   \n191                             1.0                           1.0   \n202                             1.0                           0.0   \n215                             1.0                           0.0   \n228                             0.0                           0.0   \n241                             0.0                           0.0   \n253                             0.0                           0.0   \n266                             0.0                           0.0   \n279                             0.0                           0.0   \n291                             1.0                           0.0   \n304                             1.0                           0.0   \n317                             1.0                           0.0   \n329                             1.0                           0.0   \n341                             0.0                           0.0   \n354                             0.0                           0.0   \n367                             0.0                           0.0   \n380                             0.0                           0.0   \n393                             0.0                           0.0   \n406                             0.0                           0.0   \n418                             0.0                           0.0   \n431                             1.0                           1.0   \n443                             0.0                           0.0   \n448                             0.0                           0.0   \n461                             1.0                           1.0   \n474                             0.0                           0.0   \n487                             1.0                           1.0   \n499                             0.0                           0.0   \n512                             0.0                           0.0   \n525                             0.0                           0.0   \n538                             0.0                           0.0   \n551                             1.0                           1.0   \n\n     Post_soviet_states  EU_members  Tier_2024  Visa_free_destinations  \\\n0                   1.0         0.0        2.0                   123.0   \n13                  0.0         0.0        NaN                   171.0   \n18                  1.0         0.0        2.0                    68.0   \n30                  0.0         1.0        1.0                   191.0   \n43                  1.0         0.0        2.0                    71.0   \n55                  1.0         0.0        3.0                    81.0   \n67                  0.0         1.0        1.0                   190.0   \n79                  0.0         0.0        2.0                   123.0   \n92                  1.0         0.0        2.0                   177.0   \n105                 0.0         1.0        2.0                   183.0   \n118                 0.0         1.0        1.0                   178.0   \n127                 1.0         1.0        1.0                   189.0   \n140                 0.0         1.0        1.0                   190.0   \n153                 1.0         1.0        1.0                   185.0   \n166                 0.0         1.0        1.0                   191.0   \n179                 0.0         1.0        1.0                   192.0   \n191                 1.0         0.0        NaN                   122.0   \n202                 0.0         1.0        1.0                   192.0   \n215                 0.0         1.0        2.0                   188.0   \n228                 1.0         1.0        2.0                   187.0   \n241                 0.0         0.0        1.0                   184.0   \n253                 0.0         1.0        2.0                   191.0   \n266                 0.0         1.0        2.0                   192.0   \n279                 0.0         0.0        2.0                    79.0   \n291                 1.0         1.0        2.0                   184.0   \n304                 1.0         1.0        1.0                   185.0   \n317                 0.0         1.0        1.0                   191.0   \n329                 0.0         1.0        2.2                   187.0   \n341                 1.0         0.0        2.0                   122.0   \n354                 0.0         0.0        2.0                   128.0   \n367                 0.0         1.0        1.0                   191.0   \n380                 0.0         0.0        2.0                   128.0   \n393                 0.0         0.0        2.0                   190.0   \n406                 1.0         1.0        1.0                   188.0   \n418                 0.0         1.0        2.0                   189.0   \n431                 1.0         1.0        2.0                   177.0   \n443                 1.0         0.0        3.0                   116.0   \n448                 0.0         0.0        2.2                   140.0   \n461                 0.0         1.0        2.0                   184.0   \n474                 0.0         1.0        2.0                   184.0   \n487                 0.0         1.0        1.0                   192.0   \n499                 0.0         1.0        1.0                   191.0   \n512                 0.0         0.0        2.0                   190.0   \n525                 0.0         0.0        NaN                   116.0   \n538                 1.0         0.0        2.0                   148.0   \n551                 0.0         0.0        1.0                   190.0   \n\n     GSI_gov_response_score prostitution_policy  \n0                      48.0          prohibited  \n13                      NaN                None  \n18                     42.0          prohibited  \n30                     48.0               legal  \n43                     46.0          prohibited  \n55                     37.0          prohibited  \n67                     46.0   decriminalization  \n79                     45.0        abolitionism  \n92                     43.0          prohibited  \n105                    46.0          prohibited  \n118                    45.0        abolitionism  \n127                    46.0        abolitionism  \n140                    48.0        abolitionism  \n153                    44.0        abolitionism  \n166                    47.0        abolitionism  \n179                    48.0     neoabolitionism  \n191                    48.0          prohibited  \n202                    48.0               legal  \n215                    48.0               legal  \n228                    43.0               legal  \n241                    37.0     neoabolitionism  \n253                    49.0     neoabolitionism  \n266                    46.0        abolitionism  \n279                    33.0          prohibited  \n291                    46.0               legal  \n304                    45.0          prohibited  \n317                    40.0                None  \n329                    38.0        abolitionism  \n341                    37.0          prohibited  \n354                    47.0          prohibited  \n367                    52.0               legal  \n380                    45.0        abolitionism  \n393                    49.0     neoabolitionism  \n406                    43.0        abolitionism  \n418                    52.0        abolitionism  \n431                    45.0        abolitionism  \n443                    19.0          prohibited  \n448                    44.0          prohibited  \n461                    45.0        abolitionism  \n474                    44.0        abolitionism  \n487                    49.0        abolitionism  \n499                    49.0     neoabolitionism  \n512                    39.0               legal  \n525                    38.0               legal  \n538                    40.0          prohibited  \n551                    53.0        abolitionism  \n\n\nNow, merge this dataset with the aggregated metrics dataset. The output should have one row for each unique country.\n\nMLdata = aggregated_df.merge(static_vars, on=\"Country\", how=\"left\")\nMLdata.head()\n\n\n\n\n\n\n\n\nCountry\nDetected_victims_sum\nDetected_victims_slope\nConvicted_traffickers_sum\nConvicted_traffickers_slope\nProsecuted_traffickers_sum\nProsecuted_traffickers_slope\nConvictions_over_prosecutions_mean\nNumber_Repatriated_Victims_sum\nTier_mean\n...\nSubregion\nNonpunishment_policy_before2021\nNonpunishment_policy_after2021\nLimited_nonpunishment_policy\nPost_soviet_states\nEU_members\nTier_2024\nVisa_free_destinations\nGSI_gov_response_score\nprostitution_policy\n\n\n\n\n0\nAlbania\n1235.0\n2.353147\n42.0\n-4.600000\n349.0\n1.361111\n0.314332\n92.0\n2.016667\n...\nSouthern Europe\n0.0\n1.0\n0.0\n1.0\n0.0\n2.0\n123.0\n48.0\nprohibited\n\n\n1\nAndorra\n25.0\n0.000000\n10.0\n0.000000\n43.0\n-2.153846\n0.575758\n0.0\nNaN\n...\nSouthern Europe\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n171.0\nNaN\nNone\n\n\n2\nArmenia\n177.0\n-0.206294\n75.0\n-0.248918\n83.0\n-1.416667\n0.833808\n0.0\nNaN\n...\nWestern Asia\n1.0\n1.0\n1.0\n1.0\n0.0\n2.0\n68.0\n42.0\nprohibited\n\n\n3\nAustria\n1344.0\n25.339827\n134.0\n-0.843034\n1194.0\n16.659612\n0.213883\n96.0\n1.000000\n...\nWestern Europe\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n191.0\n48.0\nlegal\n\n\n4\nAzerbaijan\n792.0\n6.300699\n231.0\n0.956522\n177.0\n-0.392857\n0.762087\n0.0\nNaN\n...\nWestern Asia\n1.0\n1.0\n0.0\n1.0\n0.0\n2.0\n71.0\n46.0\nprohibited\n\n\n\n\n5 rows × 34 columns\n\n\n\nThe merged, aggregated dataset is exported as a csv file to the processed-data folder for use in exploratory data analysis and machine learning.\n\nMLdata.to_csv('../../data/processed-data/MLdataset.csv', index=False)"
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "LLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nComing up with the title of this project\nTo write an outline / structure of project tp guide me\nto help me derive a schedule for this project\nbrainstorming ways to manipulate temporal data to obey IID assumptions\neffective data visualization tools\ndeciding and implementing the best evaluation metrics for model comparisons in supervised learning"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nSummarizing my messy notes and incorrectly worded interpretations into concise sentences\n\nsuch as my notes on the Tier placement scores– the policy descriptions were long but I needed to cut this down short and concise\n\nMostly grammar check\nText summarization for literature review\nOutline for the literature review (asked what common breakdowns look like)"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nChatGPT was consulted for:\n\nparsing data using the World Bank API\nusing BeautifulSoup to partse Wikipedia data for the Henley Passport\n\nlots of complications and trial and error\nhelped me debug the intricacies in the HTML\n\nwriting out the ISO codes to represent and map to European countries\nDEBUGGING in many parts\nMarkdown syntax and shortcuts that I’d forgotten (footnotes and in text links)\nlinking datasets to qmd\nhow to retrieve accidentally deleted files in github\nimplementing the data manipulation to turn temporal data into static\n\nClaudeAI was consulted for:\n\ncreating the interactive maps in the EDA section using plotly.express\n\nimplementing the ISO codes for mapping\ncreating color mappings\njust introducing me to the tool\n\ndeciding and implementing the best evaluation metrics for model comparisons in supervised learning\nvisualizing the results in supervised learning\nhelping to write the pipeline; I fed Claude my code and it caught mistakes that were causing errors\nLOTS of debugging help"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#general",
    "href": "technical-details/llm-usage-log.html#general",
    "title": "LLM usage log",
    "section": "General:",
    "text": "General:\n\nI was confused on some hyperparameters specifics up to this point, but ChatGPT explained clearly so that I could clearly document the information in the descriptions.\nCleared up some information"
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "The goal of this project is to collect and combine a variety of country indicators that may be informational in representing its response to human trafficking. This goal was established after analysing the requirements for Tier placements in the U.S. Department of State’s Trafficking in Persons annual report, in which countries are categorized based on some rather vague characteristics. This project instead seeks to aggregate data that represents various dimensions of a country’s ability to fight human trafficking, as well as their prioritization of protecting its people from the crime. This data will include features on the financial and political stability of a country, its enactment of policies including its level of prostitution legality policy and if it outlines victim nonpunishment policy, characteristics of the strength of the nation’s passport and its judicial system, the amount of detected victims of human trafficking in that country and nationals detected abroad, and more. By collecting holistically representative and well-rounded data on features identified to be influential factors in human trafficking response through the literature review, this project seeks to provide insight into the shortcomings of current categorizations of countries and shed light on countries making considerable progress without recognition. Moreover, the insights gained may help inform improved methods for assessing country performance.\n\n\n\nHuman trafficking is the world’s fastest growing global crime, and it takes the freedom, health, well-being, and humanity of people on this earth everyday. It is a crime that is overlooked in research and government prioritization because of how complex it is to identify, intervene in, and solve. Being an underground crime, it requires special and intensive methods in order to be cracked down on.\nThere is a considerable divide between countries in Europe. Generally, countries in Western and Northern Europe are considered to be highly civilized, advanced, stable, and peaceful, whereas countries in Southern and Eastern Europe (especially those formerly part of the Soviet bloc or Yugoslavia) are known to have more geopolitical tensions and other disadvantages as a result of a history of political turmoil. Human trafficking tends to occur more in places that are less financially, politically, and legally stable, which results in the crime being more prevalent in Southern and Eastern Europe.\nCountries in Southern and Eastern Europe are consistently categorized lower in the Tier placements by the U.S. Department of State. However, signfiicant efforts and reforms may be being performed in those countries to combat the crime they have been unfortunately plagued with a higher rate of crime to actually combat. This means that criteria used to compare countries may not reflect the entire situation. For example, countries in Northern and Western Europe tend to be more financially apt to fund the resources to fight against human trafficking, but they don’t have to use these resources because the rate of the crime is lower. In contrast, Southern and Eastern European countries must use more of their limited funding to provide more intervention. Therefore, the motivation behind this project is to uncover if other indicator spaces may provide more comprehensive explanation of a country’s true response to human trafficking.\n\n\n\nThe main objectives of this project are to analyze data in order to:\n\nIdentify which indicators are the most important for characterizing the trends and flows of human trafficking in countries in Europe\nDetermine which indicators are the most explanatory of a government’s response to human trafficking\nProvide insights into the clarity or lack of explainability of the included data in being able to identify a country’s Tier placement\n\nBy providing these analyses, this project hopes to present a better understanding of what national characteristics may influence a country’s human trafficking trends and government response abilities."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals",
    "href": "technical-details/data-collection/overview.html#goals",
    "title": "Overview",
    "section": "",
    "text": "The goal of this project is to collect and combine a variety of country indicators that may be informational in representing its response to human trafficking. This goal was established after analysing the requirements for Tier placements in the U.S. Department of State’s Trafficking in Persons annual report, in which countries are categorized based on some rather vague characteristics. This project instead seeks to aggregate data that represents various dimensions of a country’s ability to fight human trafficking, as well as their prioritization of protecting its people from the crime. This data will include features on the financial and political stability of a country, its enactment of policies including its level of prostitution legality policy and if it outlines victim nonpunishment policy, characteristics of the strength of the nation’s passport and its judicial system, the amount of detected victims of human trafficking in that country and nationals detected abroad, and more. By collecting holistically representative and well-rounded data on features identified to be influential factors in human trafficking response through the literature review, this project seeks to provide insight into the shortcomings of current categorizations of countries and shed light on countries making considerable progress without recognition. Moreover, the insights gained may help inform improved methods for assessing country performance."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#motivation",
    "href": "technical-details/data-collection/overview.html#motivation",
    "title": "Overview",
    "section": "",
    "text": "Human trafficking is the world’s fastest growing global crime, and it takes the freedom, health, well-being, and humanity of people on this earth everyday. It is a crime that is overlooked in research and government prioritization because of how complex it is to identify, intervene in, and solve. Being an underground crime, it requires special and intensive methods in order to be cracked down on.\nThere is a considerable divide between countries in Europe. Generally, countries in Western and Northern Europe are considered to be highly civilized, advanced, stable, and peaceful, whereas countries in Southern and Eastern Europe (especially those formerly part of the Soviet bloc or Yugoslavia) are known to have more geopolitical tensions and other disadvantages as a result of a history of political turmoil. Human trafficking tends to occur more in places that are less financially, politically, and legally stable, which results in the crime being more prevalent in Southern and Eastern Europe.\nCountries in Southern and Eastern Europe are consistently categorized lower in the Tier placements by the U.S. Department of State. However, signfiicant efforts and reforms may be being performed in those countries to combat the crime they have been unfortunately plagued with a higher rate of crime to actually combat. This means that criteria used to compare countries may not reflect the entire situation. For example, countries in Northern and Western Europe tend to be more financially apt to fund the resources to fight against human trafficking, but they don’t have to use these resources because the rate of the crime is lower. In contrast, Southern and Eastern European countries must use more of their limited funding to provide more intervention. Therefore, the motivation behind this project is to uncover if other indicator spaces may provide more comprehensive explanation of a country’s true response to human trafficking."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "The main objectives of this project are to analyze data in order to:\n\nIdentify which indicators are the most important for characterizing the trends and flows of human trafficking in countries in Europe\nDetermine which indicators are the most explanatory of a government’s response to human trafficking\nProvide insights into the clarity or lack of explainability of the included data in being able to identify a country’s Tier placement\n\nBy providing these analyses, this project hopes to present a better understanding of what national characteristics may influence a country’s human trafficking trends and government response abilities."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hidden Numbers: An Exploration of Factors that Influence Human Trafficking Detection and Government Response in Europe",
    "section": "",
    "text": "Human trafficking is the fastest growing criminal industry in the world. As defined by the United Nations Office on Drugs and Crime, human trafficking is the recruitment, transportation, transfer, harboring or receipt of people through force, fraud or deception, with the aim of exploiting them for profit. 27.6 million people worldwide are estimated to be victims of human trafficking at this moment, but given the complex and underground nature of the crime, it is difficult to know what the true prevalence is.\nThe complex nature of the crime implicates the need for multi-faceted, comprehensive approaches to assessing problems in its domain. In particular, evaluating how well governments are prepared to stop human trafficking and how much effort they put into saving people from the crime must be approached with careful consideration of many indirect indicators. By leveraging data science techniques for data collection, cleaning, visualization and analysis, as well as methods in unsupervised and supervised machine learning, this project aims to shed light on what indicators may be influential to explaining human trafficking trends and government responses in a country, promoting further research into those areas. To perform this task, extensive research was conducted to gain information on what variables are impactful for human trafficking. The literature review synthesizing the main themes, drivers, and complications of human trafficking can be found below.\nTo this end, this project was guided by these five research questions:"
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Hidden Numbers: An Exploration of Factors that Influence Human Trafficking Detection and Government Response in Europe",
    "section": "Literature review",
    "text": "Literature review\nThe following literature review summarizes key findings and similarities across published peer-reviewed research, scholarly articles, and policy reports on the topic of human trafficking in Europe, as well as information on the content of EU-wide policy and the U.S. Department of State’s (USDS) Trafficking in Persons Annual Report of Tier placements. Articles and reports were found through searching varying combinations of the key terms “human trafficking”, “europe”, “law enforcement”, and “policy” into both the database Google Scholar as well as the general Google search bar. Studies that met the inclusion criteria for this literature review discussed one or more of the following: 1.) trends of human trafficking in Europe, including drivers, population characteristics, and prevalence, 2.) government response to human trafficking in countries in Europe, 3.) specific comparisons or descriptions or policies pertaining to human trafficking, 4.) discussion of the USDS’s Tier placement methods, or 5.) discussion of EU-wide policy. Eight pieces of literature were ultimately included, which reflects the scarcity of research and information published on this topic. Across the variety of sources reviewed, the following themes were identified as common topics of research and findings: the characteristics of human trafficking victims in different countries in Europe, the implementation and effectiveness of EU and country-specific policies, the determinants and drivers of human trafficking in a country, and challenges to addressing trafficking.\nHuman trafficking trends in Europe\nAs human trafficking is a complex, underground crime, it is difficult to perform research that holistically represents the true story of human trafficking trends. However, the EUROSTAT Report of 2022 on Trafficking in human being statistics aimed to provide some insight into available characteristics by analysing the nationalities of detected victims in the location in which they were detected. The report found that across the entire EU in 2022, 25.1 % of registered victims came from the reporting country while 11.8 % were from other EU countries and 63.1 % from non-EU countries. The report also found that human trafficking victims’ citizenship considerably varies across the EU. The reported detected victims from countries such as Bulgaria, Romania, Hungary, Slovakia, Croatia, Latvia, Lithuania, and Czechia, which are all located in Eastern Europe, were mainly citizens of the country in which they were detected. However, in 2022, more than 80 % of registered victims in Denmark, Finland, Portugal, Spain, Malta, Greece, Belgium, Luxembourg, Italy and Estonia came from non-EU countries.1 This delivers crucial insights into not only the government responses to human trafficking, but also how the conditions in countries may drive human trafficking in their own population. This calls for investigation into how open border policy, the acceptance of refugees, and internal political stability influence national human trafficking trends. However, the EU Parliament’s 2023 briefing on human trafficking brought into discussion how trends in human trafficking are likely to change unprecendently in the wake of the extreme conditions of COVID-19 and the war in Ukraine in the last few years.2 The briefing emphasizes the need for coordinated efforts among EU Member States to address the increased risks of trafficking as a result of these crises and warns that new methods of tackling human trafficking may need to be adapted.\nDeterminants and drivers of trafficking\n\n\nAcross the research, several indicators were identified as potentially influencing human trafficking. A Swedish Masters’ thesis found that the type of prostitution policy and degree if law enforcement play roles in driving the inflow of human trafficking victims. Prostitution policy varies across European countries, with five main categories of policy type: legal, abolitionism, neoabolitionism, decriminalization, and prohibition.3 However, this contrasts with a study by4 from 2015 which finds that nations with policies that legalize or regulate prostitution do not have significantly different trafficking flows. Rather, institutional enforcement and governance play a more critical role in limiting exploitation in the prostitution market. This study finds that weaker institutional quality (including political stability and the corruption in the justice system and law enforcement) significantly facilitates trafficking in a country– because traffickers prefer to operate in place with low detection and prosecution risks.\n\n\n\n\n\n\nThis study also emphasizes that border policies play an important role in determining trafficking flow and prevalence in a country, finding that countries with visa-free short-term entry from source countries experience increased trafficking flows, but this effect diminishes if long-term stays are allowed. Another study from 2017 by5 finds that higher rates of trafficking are predicted in nations where immigrants are a larger share of the population, as well as when there is access to the sea, the GDP per capita is low, and the level of unemployment is high. When controlled for these indicators, the study also finds that legalized prostitution increases the rate of human trafficking. These varying and contradicting identified factors highlight how complex the crime of human trafficking is and how important it is to assess trafficking outcomes from a comprehensive approach.\nChallenges to addressing human trafficking\n\n\n\n\n\n\nA few research articles evaluated and discussed the challenges to addressing human trafficking in Europe– specifically, how to identify victims and prosecute traffickers. One study brought into conversation how aside from judicial, political, population and financial indicators, personal and cultural biases affect human trafficking responses. The study discussed how criminal justice actors’ decisions about human trafficking cases (identifying true victims or prosecuting true offenders) is influenced by perceptions of if the victim aligns with stereotypes of a “genuine victim”, and victims not conforming to this stereotype (specifially in cases like forced prostitution) may be misclassified and not given the care and help they need.6 This only perpetuates the trend of victims to be hesitant coming forwards to seek help for their situation. This review article also emphasized its finding that prosecution of human trafficking cases is still highly dependent on the willingness or ability of a victim to cooperate, which often faces challenges due to mistrust in the law enforcement system.\n\n\nAnother study further supports this notion, emphasizing how the fear of prosecution prevents many victims from reporting their exploitation to law enforcement, causing lower detection rates of trafficking cases.7 The authors note that this fear is compounded by traffickers’ deliberate use of threats, such as telling victims they will not be believed or that they will face jail time or deportation. The study suggests that a lack of training and awareness among law enforcement officers further exacerbates this issue, as many are unable to distinguish between offenders and trafficking victims coerced into criminal activity. The implementation of victim nonpunishment policy, which was discussed across the literature and unpacked in more detail in the next theme, could help mitigate the problem of victims’ apprehension of approaching law enforcement. A 2016 study dicusses how cross-border cooperation and collaboration are necessary to mitigate cross-border challenges in prosecution and law enforcement efforts.8 This is an important problem, because previous research highlighted that a share of trafficking victims detected within a country are often foreigners.\nPolicy implementation and effectiveness\nAccording to the reviewed literature, adherence to EU and international policy guidelines and the appropriate implementation of measures is a considerable concern for European countries’ responses to human trafficking. A study conducted at Tillburg University discusses how although the nonpunishment principle for victims of trafficking is embedded in EU law, its practical application is inconsistent.9 The nonpunishment principle is the practice of victim immunity from prosecution when coming forward, reporting the crime of human trafficking, and cooperating with law enforcement through the process of the investigation. As victims of human trafficking often are forced into illegal actions (such as prostitution, the distribution of drugs, and undocumented immigration), many victims harbor the fear that they if they come forwards to law enforcement, they will face punishment for the crimes they were forced to commit.7 provides examples from GRETA (The Council of Europe’s Group of Experts on Action against Trafficking in Human Beings) reports in which countries with strong non-punishment policies (such as Germany and Cyprus) demonstrate better victim identification and cooperation compared to those without explicit protections. However, this study also finds that even in countries with non-punishment policies for criminal offenses, victims may still face administrative penalties (e.g., deportation for undocumented migration), which undermines their ability to report crimes or access support services.\nU.S. Department of State Tier Placements\nLastly, the U.S. Department of State’s 2024 Trafficking in Persons Report was reviewed to gain insight on the current global state of human trafficking as well as more detailed summary information on the context in Europe.10 Additionally, the Tier placements were assessed, along with their definition criteria. According to the report, Tier rankings reflect an assessment of whether a country satisfies or meets the Trafficking Victims Protection Act of 2000 (TVPA) criteria (which are summarized below; see the full report for more detailed descriptions of the requirements):\nEnactment of laws prohibiting secere forms of trafficking in persons Proactive victim identification measures with systematic procedures to guide law enforcement and other government-supported front-line responders in the process of victim identification The extent to which a government ensures the safe, humane, and, to the extent possible, voluntary repatriation and reintegration of victims Government funding and partnerships with NGOs to provide victims with access to primary health care, counseling, and shelter, allowing them to recount their trafficking experiences to trained counselors and law enforcement in an environment of minimal pressure Victim protection efforts that include access to services and shelter without detention and with legal alternatives to removal to countries in which victims would face retribution or hardship Governmental efforts to reduce the demand for commercial sex acts and extraterritorial sexual exploitation and abuse\nBased on these criteria, countries are categorized in the four Tiers if they:\n\nTier 1: fully meet the TVPA’s minimum standards\nTier 2: do not fully meet the TVPA’s minimum standards but are making significant efforts to bring themselves into compliance with those standards\nTier 2 Watchlist: they satisfy the same conditions as Tier 2, AND are characterized by one of the following:\n\nthe estimated number of victims of severe forms of trafficking is very significant or is significantly increasing and the country is not taking proportional concrete actions\nthere is a failure to provide evidence of increasing efforts to combat severe forms of trafficking in persons from the previous year, including increased investigations, prosecutions, and convictions of trafficking crimes, increased assistance to victims, and decreasing evidence of complicity in severe forms of trafficking by government officials\n\nTier 3: do not fully meet the TVPA’s minimum standards and are not making significant efforts to do so\n\nOverall, reviewing the TVPA minimum standards demonstrates that the criteria are very vague for determining a country’s Tier placement. This raises alarms in the context of the findings in the Tillburg University research, which noted that the implementation of policies is not always consistent or achieved even if written into their greater policy.9 So, these vague Tier placement criteria bring into question how stringent the countries actually are in meeting these criteria– they could claim that they have implemented the appropriate policies, but there is no way to know if those policies are exercised accordingly.\n\nSummary and call to action\nOverall, this literature review has synthesized some key findings across the various forms of literature:\nThere are a multitude of factors that may influence and drive human trafficking, notably prostitution policy, victim nonpunishment policy, political, socioeconomic, institutional and criminal justice stability, the nation’s passports’ power, and the quality of law enforcement. The greatest challenges to addressing human trafficking is the lack of reporting, as a result of the lack of victims who feel comfortable coming forwards to law enforcement Human trafficking is a transnational, multidimensional problem that requires comprehensive approaches in order to be addressed The USDS’s tiering system is very vague, leading to the possibility of underfitting (underestimating the truth of) a nation’s true efforts made to combat human trafficking Overall, the research calls for the implementation and practice of victim nonpunishment policy throughout Europe\nThis literature review has inspired the inclusion of a multitude of variables in the project, as well as a fresh perspective on considerations when working with such complex data."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Hidden Numbers: An Exploration of Factors that Influence Human Trafficking Detection and Government Response in Europe",
    "section": "References",
    "text": "References\n\n\n1. Commission, E. Trafficking in human beings statistics. EUROSTAT Report (2022).\n\n\n2. Parliament, E. Briefing on human trafficking. https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/690616/EPRS_BRI(2021)690616_EN.pdf (2023).\n\n\n3. Johansson, K. The impact of prostitution policy on human trafficking. (University of Gothenburg, 2016).\n\n\n4. Hernandez, D. & Rudolph, A. Modern day slavery: What drives human trafficking in europe? European Journal of Political Economy (2015).\n\n\n5. Tallmadge, R. & Gitter, R. J. The determinants of human trafficking in the european union. Journal of Human Trafficking (2017).\n\n\n6. Cooper, F. I., Bemmel, S. R. van, Leun, J. P. van der & Kunst, M. J. J. Factors that influence the criminal justice response to human trafficking: A systematic review of north american and european studies. Crime, Law and Social Change (2024).\n\n\n7. Rodriguez-Lopez, S. The non-punishment of human trafficking victims in europe: A comparative perspective. New Journal of European Criminal Law (2024).\n\n\n8. Bosma, A. & Rijken, C. Key challenges in the combat of human trafficking: Evaluating the EU trafficking strategy and EU trafficking directive. New Journal of European Criminal Law (2024).\n\n\n9. Rijken, C. R. J. J. & Bosma, A. K. A review of the implementation of the EU strategy on human trafficking by EU members. http://trace-project.eu/wp-content/uploads/2014/11/TRACE_D1.1_Final.compressed.pdf (2014).\n\n\n10. U.S. Department of State. 2024 trafficking in persons report. https://www.state.gov/reports/2024-trafficking-in-persons-report/ (2024)."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Human trafficking remains the fastest-growing crime on the planet, impacting millions of lives worldwide. Due to its underground nature, identifying and combating this crime requires robust and comprehensive intervention methods. The U.S. Department of State’s Trafficking in Persons (TIP) report annually categorizes countries into Tiers based on their government response. However, the criteria for these placements can appear vague and may fail to recognize countries making significant progress despite structural challenges. For example, countries in Southern and Eastern Europe, burdened with historical political instability and economic limitations, face higher rates of human trafficking. Despite their efforts to address the issue, Tier placements may undervalue their progress, especially when compared to wealthier, more stable nations in Northern and Western Europe.\n\nIn this project, various indicators collected from independent sources were combined, analyzed, and modeled together with the aim of drawing a better understanding of what variables may explain factors in the problem arena of the world’s fastest growing and most complex underground crime, human trafficking. Particularly, this project aims to shed light on what indicators reflect a country’s government response to human trafficking, including how much of an effort is made with the available resources, how prepared and ethically-driven the law enforcement is, and what policies have been passed. This project deploys various advanced data science techniques and methods to aggregate data and evaluate it. By doing so, this project hopes to pave a new framework for research in the realm of human trafficking and policy evaluation.\nDetailed descriptions of the methods used in this project can be found in the data collection section of the technical details."
  },
  {
    "objectID": "report/report.html#introduction",
    "href": "report/report.html#introduction",
    "title": "Final Report",
    "section": "",
    "text": "Human trafficking remains the fastest-growing crime on the planet, impacting millions of lives worldwide. Due to its underground nature, identifying and combating this crime requires robust and comprehensive intervention methods. The U.S. Department of State’s Trafficking in Persons (TIP) report annually categorizes countries into Tiers based on their government response. However, the criteria for these placements can appear vague and may fail to recognize countries making significant progress despite structural challenges. For example, countries in Southern and Eastern Europe, burdened with historical political instability and economic limitations, face higher rates of human trafficking. Despite their efforts to address the issue, Tier placements may undervalue their progress, especially when compared to wealthier, more stable nations in Northern and Western Europe.\n\nIn this project, various indicators collected from independent sources were combined, analyzed, and modeled together with the aim of drawing a better understanding of what variables may explain factors in the problem arena of the world’s fastest growing and most complex underground crime, human trafficking. Particularly, this project aims to shed light on what indicators reflect a country’s government response to human trafficking, including how much of an effort is made with the available resources, how prepared and ethically-driven the law enforcement is, and what policies have been passed. This project deploys various advanced data science techniques and methods to aggregate data and evaluate it. By doing so, this project hopes to pave a new framework for research in the realm of human trafficking and policy evaluation.\nDetailed descriptions of the methods used in this project can be found in the data collection section of the technical details."
  },
  {
    "objectID": "report/report.html#key-findings",
    "href": "report/report.html#key-findings",
    "title": "Final Report",
    "section": "Key findings",
    "text": "Key findings\nThe EDA phase provided key insights into how various indicators interact with each other and their relationship to TIP Tier placements:\n\nThe criminal justice score emerged as a critical variable, correlating with other factors relevant to human trafficking response. Additionally, subregion and the Henley passport index (which represents the number of Visa-free destinations accessible with the national passport) were found to be influential.\nSurprisingly, indicators like victim nonpunishment policies and GSI government response scores were independent of Tier placements and criminal justice scores.\n\nVictim nonpunishment policies were repeatedly discussed in the literature reviewed for this project as a critical net of safety to ensure the cooperation and trust between trafficking victims and law enforcement. The fact that victim nonpunishment policy specifications may not carry significant weight in existing classifications implicates a more nuanced approach to understanding how to facilitate a safer environment for trafficking victims to help law enforcement combat human trafficking.\nFurthermore, the lack of similarity and correlation between the Walk Free’s Global Slavery Index (GSI) government response score and the USDS’s Tier placements is alarming, as both of these metrics aim to represent how well a government is committing to combatting human trafficking. These results suggest that further comparison of the two methods should be researched.\nThe unsupervised learning phase of the project demonstrated that the reduced-dimensional representation of the data provided limited insights. The clustering analysis revealed that the data does not exhibit strong natural groupings, as observed through the dimensionality reduction visualizations. Additionally, the number of clusters identified varied significantly across K-Means, DBSCAN, and Hierarchical Clustering, suggesting a lack of consistent structure and highlighting the complexity and sparsity of the dataset. This inconsistency may be attributed to the inherent noise, high dimensionality, and potential interdependencies among the indicators.\nIn the supervised learning phase, similar challenges emerged. The indicators failed to adequately explain the variation in target variables in the binary and multiclass classification and regression analyses, resulting in poor predictive performance for all models. However, among the types of supervised learning performed, the data was more useful for predicting binary classification, as the models compared in that analysis performed less poorly than the others. It is important to note that the data was intrinsically limited in its size and varying group sizes, which will be discussed further in the limitations section.\nThese findings highlight the complexity of human trafficking data and suggest that conventional machine learning techniques may not be sufficient in their current form to uncover underlying relationships. The limited success of both the unsupervised and supervised phases indicates a need for further data preprocessing, such as feature engineering to capture more meaningful indicators, and the inclusion of larger, more diverse datasets. Future iterations may also benefit from applying advanced techniques like neural networks or ensemble methods capable of handling complex, non-linear interactions between variables."
  },
  {
    "objectID": "report/report.html#limitations",
    "href": "report/report.html#limitations",
    "title": "Final Report",
    "section": "Limitations",
    "text": "Limitations\nThis project faced three primary limitations: the size of the final dataset, the time frame and resources available, and the inherent complexity of human trafficking as a crime.\nThe final dataset consisted of only 46 rows, corresponding to the 46 European countries. This small sample size posed significant challenges when applying machine learning algorithms. Small datasets limit a model’s ability to generalize, as they provide insufficient data to train on. This increases the risk of overfitting, where the model learns patterns specific to the training data but fails to perform well on unseen data. Additionally, the models’ performance metrics can fluctuate widely, making it difficult to derive reliable insights. While the inclusion of temporal data and the application of neural network algorithms like Long Short-Term Memory (LSTM) networks or AutoRegressive Integrated Moving Average (ARIMA) models could have provided deeper insights into time-dependent trends in human trafficking, this approach was beyond the project’s scope. The focus was limited to supervised and unsupervised learning frameworks rather than time series analyses. Future iterations of this work would benefit from incorporating longitudinal datasets and leveraging temporal machine learning techniques to capture dynamic changes in country-level indicators.\nThe project’s time constraints and resource availability also imposed limitations. Collecting, cleaning, and aggregating reliable data on human trafficking is inherently time-consuming due to its fragmented and inconsistent availability across countries. Despite these challenges, the project relied on the most comprehensive indicators available within the specified timeline. But with additional resources and a longer duration, the dataset likely could have been further expanded and more complex machine learning techniques could have been explored and applied.\nAnd lastly, the intrinsic complexity and hidden nature of human trafficking presented significant challenges to this analysis. Human trafficking is an underground crime, meaning its true prevalence is impossible to measure accurately. Officially reported victim statistics are only a fraction of the actual incidents, as many cases go undetected or unreported due to fear, stigma, or lack of resources. This underreporting introduces substantial bias into the data, limiting its representativeness. Moreover, human trafficking is influenced by an array of interconnected factors, including socioeconomic conditions, political stability, cultural norms, and legislative effectiveness. The multifaceted nature of the crime makes it difficult to isolate clear relationships between country-level indicators and human trafficking trends. Even with the best available data, the results are inherently limited by the unknown and unmeasurable variables that influence the crime. This was found apparent in the inconsistent and unreliable and unexpected correlations or lack thereof between variables in this project’s data."
  },
  {
    "objectID": "report/report.html#future-research",
    "href": "report/report.html#future-research",
    "title": "Final Report",
    "section": "Future research",
    "text": "Future research\nThe limitations of this study themselves represent significant findings, underscoring the inherent complexity of addressing human trafficking and the challenges in approaching this multifaceted issue. This project calls for future research to address the limitations experienced in this study. This includes:\n\nMitigating the small dataset size issue by integrating longitudinal (temporal) data and increasing the number of observations through global or regional comparisons\nApplying neural networks suitable to time-series data to create more reliable models and also provide insight into trends over time\nContinue researching and finding data on additional variables to account for unobserved drivers of human trafficking in order to broaden the current and past research vision\nPerform Natural Language Processing (NLP) comparative analysis on the official policies addressing human trafficking in different countries. This would necessitate advanced and sophisticated technology and methods in order to compare policies across languages.\n\nThese suggestions provide actionable and realistic pathways for advancing research and developing a more nuanced understanding of human trafficking, contributing to incremental progress toward a safer and more just world."
  },
  {
    "objectID": "report/report.html#conclusion",
    "href": "report/report.html#conclusion",
    "title": "Final Report",
    "section": "Conclusion",
    "text": "Conclusion\nThis project has provided valuable insights into the complexities of human trafficking and the challenges associated with evaluating government responses through data science. Despite the limitations of a small dataset, limited resources, and the inherent intricacies of human trafficking as an underground crime, the analyses underscored the critical need for more robust and comprehensive frameworks to address these issues. The results emphasize the need for further investigation into the alignment and effectiveness of current evaluation methods, such as the TIP report and the Global Slavery Index, to ensure a more accurate representation of country-level efforts. By addressing the limitations posed in this report and pursuing continued research, future studies can contribute to the development of more nuanced tools for evaluating human trafficking responses, ultimately aiding in the fight against this devastating global issue."
  },
  {
    "objectID": "report/report.html#acknowledgements",
    "href": "report/report.html#acknowledgements",
    "title": "Final Report",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank Dr. James Hickman and Dr. Jeff Jacobs of Georgetown University’s Data Science and Analytics program for their unwavering support and valuable insights."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\nThis project was limited in regards to numerous data considerations, including:\n\nThe reports of detected human trafficking victims are likely inaccurate and underrepresentative of the true scope of the crime\nThe complexity of interpreting data on trafficking counts, convictions, and prosecutions: It is difficult to determine whether low detected human trafficking victim counts reflect a safer country with a lower prevalence of the crime, or if it signifies a country whose law enforcement is not doing well in identifying and intervening in human trafficking cases\nManual extraction and input of policy data: Since much of the policy information was found in the form of written policy reports, the information was manually input into a CSV file. This provides both strengths and weaknesses– the benefit is that the data input was carefully supervised, ensuring accuracy of data collection. The disadvantage is that it makes the data used in this project irreproducible for future iterations.\nDataset size for machine learning\n\nComputational and analytical issues:\n\nWhen performing supervised learning analysis, some of the methods attempted return blank results, meaning that the algorithms were unable to output any predictions and evaluation metrics on the data. This problem occurred with the applications of Gradient Boosting and Support Vector Regression models to the data. This signifies that the data does not have enough information for the methods to even be able to train a model on.\n\n\n\n\n\n\nThis project requires better data in order to have any truly significant technical results and impact; the data is simply too small and unrelated to provide any meaningful insight\nThus, research should definitely look into applying LSTM or ARIMA to the data compiled in the first part of the data collection process of this project, which contained larger data\nResearch could also look into expanding beyond just European countries"
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\nThis project was limited in regards to numerous data considerations, including:\n\nThe reports of detected human trafficking victims are likely inaccurate and underrepresentative of the true scope of the crime\nThe complexity of interpreting data on trafficking counts, convictions, and prosecutions: It is difficult to determine whether low detected human trafficking victim counts reflect a safer country with a lower prevalence of the crime, or if it signifies a country whose law enforcement is not doing well in identifying and intervening in human trafficking cases\nManual extraction and input of policy data: Since much of the policy information was found in the form of written policy reports, the information was manually input into a CSV file. This provides both strengths and weaknesses– the benefit is that the data input was carefully supervised, ensuring accuracy of data collection. The disadvantage is that it makes the data used in this project irreproducible for future iterations.\nDataset size for machine learning\n\nComputational and analytical issues:\n\nWhen performing supervised learning analysis, some of the methods attempted return blank results, meaning that the algorithms were unable to output any predictions and evaluation metrics on the data. This problem occurred with the applications of Gradient Boosting and Support Vector Regression models to the data. This signifies that the data does not have enough information for the methods to even be able to train a model on."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "This project requires better data in order to have any truly significant technical results and impact; the data is simply too small and unrelated to provide any meaningful insight\nThus, research should definitely look into applying LSTM or ARIMA to the data compiled in the first part of the data collection process of this project, which contained larger data\nResearch could also look into expanding beyond just European countries"
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "All of the data used in this project was stored in CSV files in tabular format.\nData collection methods for this project included manual csv downloads, manual data entry, and extracting data from online sites using public APIs and parsing webpages using BeautifulSoup. The variables included in the dataset are listed below, alongside the source from which they were extracted and the method in which they were extracted.\nAll of the data from the UNODC was downloaded from dataUNDOC’s page on trafficking in persons, which contains the world’s most official reports on human trafficking variables.\nThe World Bank’s public API was used to extract information on relevant factors influencing the human trafficking responses and patterns identified from the literature review. The base url of the API takes two inputs: the country ISO code and the indicator code. I manually found the datasets on their web pages and copied their indicator abbreviation codes from the URL of the page. These indicators were stored in a dictionary called indicators so they could be looped as inputs to the base url and their data fetched. I also specified the country ISO codes so that data would only be extracted for countries in Europe and Central Asia. ChatGPT was consulted to create the list of countries, given my specified geographic region. I then confirmed with the International Organization for Standards’s official website. Since I was running into trouble extracting the data in an appropriate format on my own, ChatGPT was also consulted for specifying the parameters and specifications in the fetch_indicator_data function, which helped me work around the data being stored in the second list.\nPython’s package BeautifulSoup was also used to parse data from two Wikipedia pages on the USDS Tier placement over time and Henley Passport Index per country. BeautifulSoup accomplishes web scraping by parsing the HTML content of the webpage. It accesses the raw HTML content of the webpage and extracts the specified information through its creation of a “parse tree”. Additional methods such as find(), find_all(), or tag navigation can be used to locate and extract the desired data.\nThe variables included in the preliminary merged dataset to be manipulated in the data cleaning phase contained these variables:\n\nCountry\nYear\nDetected victims of human trafficking (UNODC)\nHuman trafficking convictions per country (UNODC)\nHuman trafficking prosecutions per country (UNODC)\nPopulation (World Bank)\nGDP per capita (World Bank)\nUnemployment rate (World Bank)\nCriminal justice score (World Bank)\nPolitical stability (World Bank)\nPercentage of population that are refugees (World Bank)\nTier placement in 2024 (2024 USDS Trafficking in Persons report)\nTier placements every year from 2013-2023 (Wikipedia)\nVictim nonpunishment policy (manually extracted and input into dataset from GRETA’s non-punishment principle report)1\nProstitution policy (manually extracted and input into dataset from the European Parliament)\nNumber of visa-free destinations with national passport (also known as Henley passport index) (Wikipedia)\nEU membership (manually extracted and input into dataset from the E.U. official site)\nPost-soviet membership (manually extracted and input into dataset from Wikipedia)\nGSI Government Response Score\n\nThe Global Slavery Index (GSI) dataset is codified and presented by Walk Free, an international human rights-based nonprofit that streamlines research and calls for policy reform into modern slavery, an umbrella term that encompasses exploitation, forced labour, human trafficking and forced marriage\nThe government response score is one metric developed in the GSI dataset among many others. The score is based on the country’s status on dozens of activity criteria fulfillments\nTo extract the GSI government response score per country, the Total accumulation of points of criteria satisfaction was taken from the sheet in the dataset with the results\n\n\nThe raw datasets can be found here:\nUNODC data World Bank data Visa free destinations data Tier placements over time Policy data (all manually extracted) GSI Government response score\nThe resulting CSV file with all of this combined data had 565 rows for each unique combination of country and year. While some variables (namely, the data from the UNODC, World Bank, and Tier placement over time) had data over time, the rest of the indicators were static variables– meaning a single value was extracted per country. These values were repeated for every combination of year and country, per country.\nHowever, temporal data of this kind violates the I.I.D. (independently and identically distributed) assumption of data for use in machine learning algorithms. In order to be able to use this data for supervised and unsupervised learning, the temporal data had to be manipulated into single point estimates characterizing the indicator’s nature over time. This was completed in the data cleaning section of the project.\n\n\n\nVarious functions in from python’s pandas package were deployed for data cleaning: the standardization of country naming, the merging of different datasets, the aggregation of temporal data into single metrics for input into machine learning models, and more.\nCountry names were standardized to common country title representations by first comparing the differing denotations between the UNODC and World Bank datasets, and then manually writing a mapping tool to change any instance of a certain presentation of the country title to the preferred one. For example, the Czech Republic was listed as “Czechoslovakia” in some sources, and Turkey denoted as “Turkiye”.\nUsing pandas tools, the temporal data was aggregated to single metrics for input into the machine learning models to represent either the rate of change of the variable over time, the mean value over time, or the sum of values. Some variables included more than one aggregate representation. ChatGPT was consulted in the process of brainstorming how to approach this data manipulation strategy by prompting the LLM with the question: “What are the best ways to force data with temporal component to obey IID assumptions?” The rate of change was calculated as the regression slope between each successive change between consecutive years using the scipy.stat's linregress tool in python. The final dataset for input into the machine learning models thus contained the following variables. The shape of the dataset is (46,43), indicating 46 rows for each unique country and 43 final indicators. Any variable titles ending in _sum represent the total accumulated count of the value over the years. Any ending in _mean represent the mean value over the years. Any ending in _slope represent the general regression slope of the value over the years, for which a positive value indicates that the indicator has been increasing over the years.\nThe following variables composed the final, aggregated dataset for input into the machine learning models:\n\nCountry\nSubregion\nDetected_victims_sum\nDetected_victims_slope\nConvicted_traffickers_sum\nConvicted_traffickers_slope\nProsecuted_traffickers_sum\nProsecuted_traffickers_slope\nConvictions_over_prosecutions_mean\nNumber_Repatriated_Victims_sum\nTier_mean\nTier_slope\nCriminal_justice_mean\nCriminal_justice_slope\nGDP_per_capita_mean\nGDP_per_capita_slope\nPolitical_stability_mean\nPolitical_stability_slope\nPopulation_mean\nRefugee_population_mean\nRefugee_population_slope\nUnemployment_rate_mean\nUnemployment_rate_slope\nDetections_per_100_mean\nReported_Years_Detected_Victims\nNonpunishment_policy_before2021\nNonpunishment_policy_after2021\nLimited_nonpunishment_policy\nPost_soviet_states\nEU_members\nVisa_free_destinations\nGSI_gov_response_score\nprostitution_policy\nTier_2024\n\nDownload the final dataset\n\n\n\nThe data visualization component of this project utilizes numerous python packages, including pandas, numpy, matplotlib.pyplot, and plotly.express, including its choropleth tool for the creation of interactive, time-based maps. Interactive maps, correlation matrix heatmaps, pairplots, line graphs, bar plots and boxplots were all generated to perform visual perspectives of the data. Furthermore, statistical hypothesis tests such as the t-test and Kruskal-Wallis test with Dunn’s pairwise analysis were performed to assess the statistical significance of differences between levels of categorical variables.\n\nThe t-test is a hypothesis test that compares the means of two groups.\n\nAssumptions:\n\nNormality: the data in each group should be roughly Normally distributed\nEquality of variances: the variance of each group should be approximately the same\n\nIf this assumption can’t be confirmed, Welch’s t-test can be performed\n\n\nTypes of t-tests:\n\nOne-sample t-test: assesses if the mean of one group differs significantly from the known population mean\nIndependent two-sample t-test: Compares the means of two independent groups to determine if they are significantly different.\nPaired t-test: Compares the means of two related groups (for example, before-and-after measurements)\n\nFor the EDA in this project, Welch’s two-sample independent t-test will be performed using scipy.stats.ttest_ind\n\nThe Kruskal-Wallis test compares the medians of three or more independent groups to determine any statistically significant differences using scipy.stats.kruskal\n\nNon-parametric alternative to ANOVA\nNon-parametric: means that the test does not depend on assumptions like Normality or equality of variances\nTest statistic is calculted by the rank sums of observations in each group\nWorks well for small sample sizes or groups with varying sample sizes\nWhen the Kruskal-Wallis test detects a significant difference, Dunn’s test is used for pairwise comparisons between groups to determine which specific groups differ (using scikit_posthocs)\n\n\n\n\n\nThe supervised and unsupervised components used various tools in python’s sklearn packages for different machine learning tasks, as well as specific python packages for more specific models, such as xgboost. The models used are briefly outlined below, with more specific and detailed explanations in the respective section of the project report in which they are applied.\nUnsupervised Learning\n\nDimensionality Reduction:\n\nPrincipal Component Analysis (PCA): Reduces data dimensions by transforming features into orthogonal principal components while retaining maximum variance\nt-Distributed Stochastic Neighbor Embedding (t-SNE): Projects high-dimensional data into a lower-dimensional space, preserving local structure for visualization\n\nClustering Methods:\n\nK-Means: Partitions data into clusters by minimizing intra-cluster Euclidean distances\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN): Groups data points based on density, detecting clusters of varying shapes while labeling outliers as noise\nHierarchical Clustering: Groups data points by constructing a hierarchy of clusters using an agglomerative (bottom-up) or divisive (top-down) approach\n\n\nSupervised Learning\n\nClassification Methods:\n\nLogistic Regression: Predicts binary outcomes using a logistic function\nRandom Forest: Ensemble method using decision trees to improve accuracy and reduce overfitting\nNaive Bayes: Probabilistic classifier based on Bayes’ theorem, assuming feature independence\nXGBoost: Gradient-boosting algorithm that combines multiple weak learners for robust classification\n\nRegression Methods:\n\nLasso Regression: Linear regression with L1 regularization to enforce sparsity and prevent overfitting\nRandom Forest Regressor: Uses ensemble decision trees to model non-linear relationships in regression tasks\n\n\nAs noted, please see the respective unsupervised and supervised learning sections of this project for more detailed explanations of the models alongside their code applications.\nAll code in this project was executed using the programming language Python in Jupyter notebooks, and this website was assembled using Quarto."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#footnotes",
    "href": "technical-details/data-collection/methods.html#footnotes",
    "title": "Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Group of Experts on Action against Trafficking in Human Beings (GRETA) is responsible for monitoring the implementation of the Council of Europe Convention on Action against Trafficking in Human Beings by the Parties. It conducts research and publishes country reports evaluating government activities and performance.↩︎"
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Explore possible topics by brainstorming with GPT\nPerform background research and literature review\n\nread international / European policies or guidelines\nread and take notes on USDS’s TVPA annual report on human trafficking (get information about Tiers)\ncompile at least 10 peer reviewed sources for literature review\n\nbrainstorm data collection / necessary variables by evaluating findings from literature review\nWrite overview section\nData collection and cleaning\n\nUNODC data: download Excel file from UNODC website\nWorld Bank indicators data:\n\nuse ChatGPT to help write data fetching code\nspecify the symbols for the intended indicators\nrun code and output to csv\n\nPolicy data: manual collection\n\nTiers: manually input for each European country from USDS 2024 TVPA Report\n\nuse ChatGPT and EU website to list every country in Europe / Western Asia\n\nNonpunishment policy: manually input from GRETA report\nProstitution policy: manually input from Wikipedia page\nEU membership: manually input from EU website\nPrior Soviet bloc membership: manually input from Wikipedia page\n\nHenley passport index data: parse from Wikipedia table using BeautifulSoup\nGSI Government Response score: download from GSI website\n\nWrite methods section\nWrite overview section\nWrite overview section\nData cleaning\n\nwrite out technical methods / purposes\nEnsure that all datasets use the same name representation for countries\nMerge all datasets\n\n\nEDA\n\nCreate visualizations\nWrite a technical methods section for visualizations\nPerform statistical tests\nWrite a technical methods sections for statistical tests\nCreate interactive maps\nPerform PCA and t-SNE\nWrite a technical methods sections for PCA & t-SNE\n\nSupervised learning\n\nWrite a methods / goals overview and explain the chosen models\n\nPrepare data: split features and targets, convert to numpy array, standardize\nPerform supervised learning classification models\n\nCompare accuracy and performance and determine optimal model\n\nInterpret findings and compare to original Tiering system\n\nUnsupervised learning\n\nWrite a methods / goals overview and explain the chosen models\nPerform unsupervised clustering models (k-means, DBSCAN)\n\nCompare their accuracy and performance and determine best fitting model\n\n\nCommunicate implications\n\nLLM usage log\nWrite report\n\nIntroduction / literature review\nResults / conclusion\n\nCreate slideshow for in-person presentation\nGo back and ensure formatting, interpretations, include storytelling components (explain every step)\nWrite closing section"
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "",
    "text": "Explore possible topics by brainstorming with GPT\nPerform background research and literature review\n\nread international / European policies or guidelines\nread and take notes on USDS’s TVPA annual report on human trafficking (get information about Tiers)\ncompile at least 10 peer reviewed sources for literature review\n\nbrainstorm data collection / necessary variables by evaluating findings from literature review\nWrite overview section\nData collection and cleaning\n\nUNODC data: download Excel file from UNODC website\nWorld Bank indicators data:\n\nuse ChatGPT to help write data fetching code\nspecify the symbols for the intended indicators\nrun code and output to csv\n\nPolicy data: manual collection\n\nTiers: manually input for each European country from USDS 2024 TVPA Report\n\nuse ChatGPT and EU website to list every country in Europe / Western Asia\n\nNonpunishment policy: manually input from GRETA report\nProstitution policy: manually input from Wikipedia page\nEU membership: manually input from EU website\nPrior Soviet bloc membership: manually input from Wikipedia page\n\nHenley passport index data: parse from Wikipedia table using BeautifulSoup\nGSI Government Response score: download from GSI website\n\nWrite methods section\nWrite overview section\nWrite overview section\nData cleaning\n\nwrite out technical methods / purposes\nEnsure that all datasets use the same name representation for countries\nMerge all datasets\n\n\nEDA\n\nCreate visualizations\nWrite a technical methods section for visualizations\nPerform statistical tests\nWrite a technical methods sections for statistical tests\nCreate interactive maps\nPerform PCA and t-SNE\nWrite a technical methods sections for PCA & t-SNE\n\nSupervised learning\n\nWrite a methods / goals overview and explain the chosen models\n\nPrepare data: split features and targets, convert to numpy array, standardize\nPerform supervised learning classification models\n\nCompare accuracy and performance and determine optimal model\n\nInterpret findings and compare to original Tiering system\n\nUnsupervised learning\n\nWrite a methods / goals overview and explain the chosen models\nPerform unsupervised clustering models (k-means, DBSCAN)\n\nCompare their accuracy and performance and determine best fitting model\n\n\nCommunicate implications\n\nLLM usage log\nWrite report\n\nIntroduction / literature review\nResults / conclusion\n\nCreate slideshow for in-person presentation\nGo back and ensure formatting, interpretations, include storytelling components (explain every step)\nWrite closing section"
  },
  {
    "objectID": "technical-details/progress-log.html#progress-timeline",
    "href": "technical-details/progress-log.html#progress-timeline",
    "title": "Progress log",
    "section": "Progress Timeline",
    "text": "Progress Timeline\n11-15-2024\n\nOffice hours with Professor James to solidify ideas\n\nChose idea and committed to working over the weekend to develop it more\n\n\n11-19-2024\n\nOffice hours with Professor James to further discuss options for trajectory of the topic\n\n11-25-2024\n\nOffice hours with Professor Jeff to gain his insight from his expertise and advice on geopolitical human rights topics, and help me narrow down my research focus\n\nDecided to narrow focus to Europe\n\n\n11-27-2024\n\nGOAL: finish data collection and cleaning\n\n12-06-2024\n\nOffice hours with Professor James to discuss how to go about the temporal aspect of my data for machine learning applications\n\nFollowing his advice, I consulted an LLM on what the best options are and how to implement them\n\n\n12-07-2024\n\nGOAL: finish EDA\n\n11-27-2024\n\nGOAL: finish unsupervised learning\n\n12-10-2024 * [x] GOAL: finish supervised learning"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "The goal of this project is to collect and combine a variety of country indicators that may be informational in representing its response to human trafficking. This goal was established after analysing the requirements for Tier placements in the U.S. Department of State’s Trafficking in Persons annual report, in which countries are categorized based on some rather vague characteristics. This project instead seeks to aggregate data that represents various dimensions of a country’s ability to fight human trafficking, as well as their prioritization of protecting its people from the crime. This data will include features on the financial and political stability of a country, its enactment of policies including its level of prostitution legality policy and if it outlines victim nonpunishment policy, characteristics of the strength of the nation’s passport and its judicial system, the amount of detected victims of human trafficking in that country and nationals detected abroad, and more. By collecting holistically representative and well-rounded data on features identified to be influential factors in human trafficking response through the literature review, this project seeks to provide insight into the shortcomings of current categorizations of countries and shed light on countries making considerable progress without recognition. Moreover, the insights gained may help inform improved methods for assessing country performance.\n\n\n\nHuman trafficking is the world’s fastest growing global crime, and it takes the freedom, health, well-being, and humanity of people on this earth everyday. It is a crime that is overlooked in research and government prioritization because of how complex it is to identify, intervene in, and solve. Being an underground crime, it requires special and intensive methods in order to be cracked down on.\nThere is a considerable divide between countries in Europe. Generally, countries in Western and Northern Europe are considered to be highly civilized, advanced, stable, and peaceful, whereas countries in Southern and Eastern Europe (especially those formerly part of the Soviet bloc or Yugoslavia) are known to have more geopolitical tensions and other disadvantages as a result of a history of political turmoil. Human trafficking tends to occur more in places that are less financially, politically, and legally stable, which results in the crime being more prevalent in Southern and Eastern Europe.\nCountries in Southern and Eastern Europe are consistently categorized lower in the Tier placements by the U.S. Department of State. However, signfiicant efforts and reforms may be being performed in those countries to combat the crime they have been unfortunately plagued with a higher rate of crime to actually combat. This means that criteria used to compare countries may not reflect the entire situation. For example, countries in Northern and Western Europe tend to be more financially apt to fund the resources to fight against human trafficking, but they don’t have to use these resources because the rate of the crime is lower. In contrast, Southern and Eastern European countries must use more of their limited funding to provide more intervention. Therefore, the motivation behind this project is to uncover if other indicator spaces may provide more comprehensive explanation of a country’s true response to human trafficking.\n\n\n\nThe main objectives of this project are to analyze data in order to:\n\nIdentify which indicators are the most important for characterizing the trends and flows of human trafficking in countries in Europe\nDetermine which indicators are the most explanatory of a government’s response to human trafficking\nProvide insights into the clarity or lack of explainability of the included data in being able to identify a country’s Tier placement\n\nBy providing these analyses, this project hopes to present a better understanding of what national characteristics may influence a country’s human trafficking trends and government response abilities."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "",
    "text": "The goal of this project is to collect and combine a variety of country indicators that may be informational in representing its response to human trafficking. This goal was established after analysing the requirements for Tier placements in the U.S. Department of State’s Trafficking in Persons annual report, in which countries are categorized based on some rather vague characteristics. This project instead seeks to aggregate data that represents various dimensions of a country’s ability to fight human trafficking, as well as their prioritization of protecting its people from the crime. This data will include features on the financial and political stability of a country, its enactment of policies including its level of prostitution legality policy and if it outlines victim nonpunishment policy, characteristics of the strength of the nation’s passport and its judicial system, the amount of detected victims of human trafficking in that country and nationals detected abroad, and more. By collecting holistically representative and well-rounded data on features identified to be influential factors in human trafficking response through the literature review, this project seeks to provide insight into the shortcomings of current categorizations of countries and shed light on countries making considerable progress without recognition. Moreover, the insights gained may help inform improved methods for assessing country performance."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "",
    "text": "Human trafficking is the world’s fastest growing global crime, and it takes the freedom, health, well-being, and humanity of people on this earth everyday. It is a crime that is overlooked in research and government prioritization because of how complex it is to identify, intervene in, and solve. Being an underground crime, it requires special and intensive methods in order to be cracked down on.\nThere is a considerable divide between countries in Europe. Generally, countries in Western and Northern Europe are considered to be highly civilized, advanced, stable, and peaceful, whereas countries in Southern and Eastern Europe (especially those formerly part of the Soviet bloc or Yugoslavia) are known to have more geopolitical tensions and other disadvantages as a result of a history of political turmoil. Human trafficking tends to occur more in places that are less financially, politically, and legally stable, which results in the crime being more prevalent in Southern and Eastern Europe.\nCountries in Southern and Eastern Europe are consistently categorized lower in the Tier placements by the U.S. Department of State. However, signfiicant efforts and reforms may be being performed in those countries to combat the crime they have been unfortunately plagued with a higher rate of crime to actually combat. This means that criteria used to compare countries may not reflect the entire situation. For example, countries in Northern and Western Europe tend to be more financially apt to fund the resources to fight against human trafficking, but they don’t have to use these resources because the rate of the crime is lower. In contrast, Southern and Eastern European countries must use more of their limited funding to provide more intervention. Therefore, the motivation behind this project is to uncover if other indicator spaces may provide more comprehensive explanation of a country’s true response to human trafficking."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "",
    "text": "The main objectives of this project are to analyze data in order to:\n\nIdentify which indicators are the most important for characterizing the trends and flows of human trafficking in countries in Europe\nDetermine which indicators are the most explanatory of a government’s response to human trafficking\nProvide insights into the clarity or lack of explainability of the included data in being able to identify a country’s Tier placement\n\nBy providing these analyses, this project hopes to present a better understanding of what national characteristics may influence a country’s human trafficking trends and government response abilities."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\nThis project was limited in regards to numerous data considerations, including:\n\nThe reports of detected human trafficking victims are likely inaccurate and underrepresentative of the true scope of the crime\nThe complexity of interpreting data on trafficking counts, convictions, and prosecutions: It is difficult to determine whether low detected human trafficking victim counts reflect a safer country with a lower prevalence of the crime, or if it signifies a country whose law enforcement is not doing well in identifying and intervening in human trafficking cases\nManual extraction and input of policy data: Since much of the policy information was found in the form of written policy reports, the information was manually input into a CSV file. This provides both strengths and weaknesses– the benefit is that the data input was carefully supervised, ensuring accuracy of data collection. The disadvantage is that it makes the data used in this project irreproducible for future iterations.\nDataset size for machine learning\n\nComputational and analytical issues:\n\nWhen performing supervised learning analysis, some of the methods attempted return blank results, meaning that the algorithms were unable to output any predictions and evaluation metrics on the data. This problem occurred with the applications of Gradient Boosting and Support Vector Regression models to the data. This signifies that the data does not have enough information for the methods to even be able to train a model on."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nThis project requires better data in order to have any truly significant technical results and impact; the data is simply too small and unrelated to provide any meaningful insight\nThus, research should definitely look into applying LSTM or ARIMA to the data compiled in the first part of the data collection process of this project, which contained larger data\nResearch could also look into expanding beyond just European countries"
  },
  {
    "objectID": "technical-details/data-collection/main.html#footnotes",
    "href": "technical-details/data-collection/main.html#footnotes",
    "title": "Data Collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Group of Experts on Action against Trafficking in Human Beings (GRETA) is responsible for monitoring the implementation of the Council of Europe Convention on Action against Trafficking in Human Beings by the Parties. It conducts research and publishes country reports evaluating government activities and performance.↩︎"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised learning is a type of machine learning that aims to fit the best model for predicting values of a labeled variable. The purpose of performing supervised learning to this data is to gain insight into how strong internal connections are between data points and variables, and if they have strength in being used to estimate another variable’s value. Three types of supervised learning will be performed in different sections: Binary classification, Multi-class classification, and Regression. The model selection methods, evaluation metrics, and results will be discussed within each of those sections.\nBelow is an outline of general information applied to all models:\n\n\nIn order to be used in supervised learning, data has to be cleaned, processed and standardized following specific criteria in order to be appropriately used. The following techniques have been performed on the data for use in the models throughout this section:\nHandling missing data - Missing data points have been filled in with the mean of other datapoints in that feature. This option was chosen because this dataset is too small to remove rows with missing values in certain columns, so to preserve the size, an appropriate approximation method was used.\nHandling categorical variables - Categorical variables (Subregion and prostitution policy) were one-hot encoded in order to be represented as numerical variables and included in the supervised learning models\n - **What is one-hot encoding?**: a machine learning technique that converts categorical data into numerical data, denoted as 0 or 1. The levels of the categorical variable are pivoted so that each level becomes a column. If the row satisfies the categorical variable level, a 1 is input under that level's column, while the remaining levels fill with 0. \nStandardization - The data was scaled using Sci-kit learn’s StandardScaler, a python package tool that automatically applies z-score standardization to the data, which reduces all data points to a more standard and comparable metric\n\n\n\nAll models used a 75-25 testing-training split. All models in this section were split using the default settings of python’s SciKit-Learn Model Selection’s train_test_split tool, which splits 75% of he data into training and 25% into testing.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv('../../data/processed-data/MLdataset.csv')\ndf.columns\n\nIndex(['Country', 'Detected_victims_sum', 'Detected_victims_slope',\n       'Convicted_traffickers_sum', 'Convicted_traffickers_slope',\n       'Prosecuted_traffickers_sum', 'Prosecuted_traffickers_slope',\n       'Convictions_over_prosecutions_mean', 'Number_Repatriated_Victims_sum',\n       'Tier_mean', 'Tier_slope', 'Criminal_justice_mean',\n       'Criminal_justice_slope', 'GDP_per_capita_mean', 'GDP_per_capita_slope',\n       'Political_stability_mean', 'Political_stability_slope',\n       'Population_mean', 'Refugee_population_mean',\n       'Refugee_population_slope', 'Unemployment_rate_mean',\n       'Unemployment_rate_slope', 'Detections_per_100_mean',\n       'Reported_Years_Detected_Victims', 'Subregion',\n       'Nonpunishment_policy_before2021', 'Nonpunishment_policy_after2021',\n       'Limited_nonpunishment_policy', 'Post_soviet_states', 'EU_members',\n       'Tier_2024', 'Visa_free_destinations', 'GSI_gov_response_score',\n       'prostitution_policy'],\n      dtype='object')\n\n\n\ndf.dtypes\n\nCountry                                object\nDetected_victims_sum                  float64\nDetected_victims_slope                float64\nConvicted_traffickers_sum             float64\nConvicted_traffickers_slope           float64\nProsecuted_traffickers_sum            float64\nProsecuted_traffickers_slope          float64\nConvictions_over_prosecutions_mean    float64\nNumber_Repatriated_Victims_sum        float64\nTier_mean                             float64\nTier_slope                            float64\nCriminal_justice_mean                 float64\nCriminal_justice_slope                float64\nGDP_per_capita_mean                   float64\nGDP_per_capita_slope                  float64\nPolitical_stability_mean              float64\nPolitical_stability_slope             float64\nPopulation_mean                       float64\nRefugee_population_mean               float64\nRefugee_population_slope              float64\nUnemployment_rate_mean                float64\nUnemployment_rate_slope               float64\nDetections_per_100_mean               float64\nReported_Years_Detected_Victims         int64\nSubregion                              object\nNonpunishment_policy_before2021       float64\nNonpunishment_policy_after2021        float64\nLimited_nonpunishment_policy          float64\nPost_soviet_states                    float64\nEU_members                            float64\nTier_2024                             float64\nVisa_free_destinations                float64\nGSI_gov_response_score                float64\nprostitution_policy                    object\ndtype: object"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#general-overview",
    "href": "technical-details/supervised-learning/main.html#general-overview",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised learning is a type of machine learning that aims to fit the best model for predicting values of a labeled variable. The purpose of performing supervised learning to this data is to gain insight into how strong internal connections are between data points and variables, and if they have strength in being used to estimate another variable’s value. Three types of supervised learning will be performed in different sections: Binary classification, Multi-class classification, and Regression. The model selection methods, evaluation metrics, and results will be discussed within each of those sections.\nBelow is an outline of general information applied to all models:\n\n\nIn order to be used in supervised learning, data has to be cleaned, processed and standardized following specific criteria in order to be appropriately used. The following techniques have been performed on the data for use in the models throughout this section:\nHandling missing data - Missing data points have been filled in with the mean of other datapoints in that feature. This option was chosen because this dataset is too small to remove rows with missing values in certain columns, so to preserve the size, an appropriate approximation method was used.\nHandling categorical variables - Categorical variables (Subregion and prostitution policy) were one-hot encoded in order to be represented as numerical variables and included in the supervised learning models\n - **What is one-hot encoding?**: a machine learning technique that converts categorical data into numerical data, denoted as 0 or 1. The levels of the categorical variable are pivoted so that each level becomes a column. If the row satisfies the categorical variable level, a 1 is input under that level's column, while the remaining levels fill with 0. \nStandardization - The data was scaled using Sci-kit learn’s StandardScaler, a python package tool that automatically applies z-score standardization to the data, which reduces all data points to a more standard and comparable metric\n\n\n\nAll models used a 75-25 testing-training split. All models in this section were split using the default settings of python’s SciKit-Learn Model Selection’s train_test_split tool, which splits 75% of he data into training and 25% into testing.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv('../../data/processed-data/MLdataset.csv')\ndf.columns\n\nIndex(['Country', 'Detected_victims_sum', 'Detected_victims_slope',\n       'Convicted_traffickers_sum', 'Convicted_traffickers_slope',\n       'Prosecuted_traffickers_sum', 'Prosecuted_traffickers_slope',\n       'Convictions_over_prosecutions_mean', 'Number_Repatriated_Victims_sum',\n       'Tier_mean', 'Tier_slope', 'Criminal_justice_mean',\n       'Criminal_justice_slope', 'GDP_per_capita_mean', 'GDP_per_capita_slope',\n       'Political_stability_mean', 'Political_stability_slope',\n       'Population_mean', 'Refugee_population_mean',\n       'Refugee_population_slope', 'Unemployment_rate_mean',\n       'Unemployment_rate_slope', 'Detections_per_100_mean',\n       'Reported_Years_Detected_Victims', 'Subregion',\n       'Nonpunishment_policy_before2021', 'Nonpunishment_policy_after2021',\n       'Limited_nonpunishment_policy', 'Post_soviet_states', 'EU_members',\n       'Tier_2024', 'Visa_free_destinations', 'GSI_gov_response_score',\n       'prostitution_policy'],\n      dtype='object')\n\n\n\ndf.dtypes\n\nCountry                                object\nDetected_victims_sum                  float64\nDetected_victims_slope                float64\nConvicted_traffickers_sum             float64\nConvicted_traffickers_slope           float64\nProsecuted_traffickers_sum            float64\nProsecuted_traffickers_slope          float64\nConvictions_over_prosecutions_mean    float64\nNumber_Repatriated_Victims_sum        float64\nTier_mean                             float64\nTier_slope                            float64\nCriminal_justice_mean                 float64\nCriminal_justice_slope                float64\nGDP_per_capita_mean                   float64\nGDP_per_capita_slope                  float64\nPolitical_stability_mean              float64\nPolitical_stability_slope             float64\nPopulation_mean                       float64\nRefugee_population_mean               float64\nRefugee_population_slope              float64\nUnemployment_rate_mean                float64\nUnemployment_rate_slope               float64\nDetections_per_100_mean               float64\nReported_Years_Detected_Victims         int64\nSubregion                              object\nNonpunishment_policy_before2021       float64\nNonpunishment_policy_after2021        float64\nLimited_nonpunishment_policy          float64\nPost_soviet_states                    float64\nEU_members                            float64\nTier_2024                             float64\nVisa_free_destinations                float64\nGSI_gov_response_score                float64\nprostitution_policy                    object\ndtype: object"
  }
]